[["index.html", "Math review for new biostatistics students 1 Introduction", " Math review for new biostatistics students Patrick Breheny and Eldon Sorensen 1 Introduction This guide is intended as a review of fundamental math concepts for students who will be starting an MS or PhD program in biostatistics. More specifically, its intended audience is new students at the University of Iowa, but the material here is quite general and I would expect it to be useful to any new student in a biostatistics program regardless of where it is. Why a math review? Is math the most important skill for a statistician? Not necessarily. However, in our experience, a shaky/rusty foundation in math is the thing most likely to lead to problems in the first year of graduate school. When you encounter new statistical concepts, instructors will introduce and explain them. But the mathematical techniques this guide covers, they will assume you already know. This guide focuses in particular on two areas of mathematics, and for different reasons. Calculus, because it is a big topic &#x2013; students often take Calc I, Calc II, and Calc III. That&#x2019;s a lot of material and it&#x2019;s not clear what needs to be reviewed and what can be skipped. Also matrix algebra, because it tends not to be taught very well at the undergraduate level. Perhaps more accurately, courses tend to focus on old-fashioned topics like inverting matrices by hand and not on the kinds of manipulations that are helpful in statistics. In principle, any idea from math could come up and be helpful to a statistician. In reality, however, certain ideas come up far more often than others, and this guide focuses on topics of greatest relevance. A good example is trigonometry: this almost never comes up in statistics. There is really no need to spend any time whatsoever reviewing it prior to stating a graduate program in statistics. On the other hand, properties of exponents and logarithms come up constantly. You need to know every property, because you will use them more or less every day, and if you don&#x2019;t know them, you will be constantly making errors on all of your homework and tests. Finally, the focus here is really on the math &#x2013; as noted above, we expect to teach you the statistics once you get here. However, to help make connections, I will occasionally point out the relevance of certain concepts to the field of biostatistics. If you&#x2019;ve taken statistics in the past and terms like &#x201C;independent events&#x201D; and &#x201C;regression model&#x201D; mean something to you, great. If not, however, don&#x2019;t worry about it. You can appreciate the connection later once you learn about these ideas in graduate school. To reiterate: this guide is intended to be a concise review of main ideas in calculus. If a section is unfamiliar or confusing, it would probably be a good idea to read the corresponding section of your calculus textbook, which will have a lot more examples, explanations, graphs, etc. Also, there are very few exercises and solutions provided in this review. Obviously, exercises are extremely helpful, especially if you feel you are rusty on a particular section. We recommend either finding problems from your calculus book or purchasing a book such as Schaum&#x2019;s Outline of Calculus. Finally, if you spot any mistakes or typos, please let me know! "],["calculus.html", "2 Calculus", " 2 Calculus In this chapter, we will review/collect a large number of results that you should know and be familiar with from calculus. I&#x2019;m not going to prove them or provide a bunch of details and explanations and graphs, so if anything strikes you as unfamiliar or you want more details, please consult your calculus textbook. Calculus is important to statistics for lots of reasons, but I would like to point out three major ones before we begin the review. Finding most likely solutions A statistical analysis typically begins with some sort of model for how the data (which I&#x2019;ll call ddd) depends on an unknown parameter (which I&#x2019;ll call &#x3B8;\\theta&#x3B8;). We observe the data, but what&#x2019;s &#x3B8;\\theta&#x3B8;? To estimate it, we typically create some function (which I&#x2019;ll call fff) that is large when &#x3B8;\\theta&#x3B8; is in agreement with the data we&#x2019;ve seen. To find the &#x201C;best&#x201D; value of &#x3B8;\\theta&#x3B8;, we can take the derivative of fff to find the optimal value. Note that this is actually a partial derivative, since fff would be a function of both &#x3B8;\\theta&#x3B8; and ddd. Probability and density For continuous quantities such as height, the distribution of likely values is specified in terms of a probability density fff. Calculating the probability from a probability density involves integration. For example, if we wanted to know the probability that a person&#x2019;s height was between 63 and 66 inches: &#x222B;6366f(x)&#x2009;dx.\\int_{63}^{66} f(x) \\, dx .&#x222B;6366&#x200B;f(x)dx. Independent observations Suppose we are interested in the probability of events A1,A2,&#x2026;,AnA_1, A_2, \\ldots, A_nA1&#x200B;,A2&#x200B;,&#x2026;,An&#x200B;. If those events are independent, this is given by P(A1)P(A2)&#x22EF;P(An)=&#x220F;i=1nP(Ai).P(A_1) P(A_2) \\cdots P(A_n) = \\prod_{i=1}^n P(A_i) .P(A1&#x200B;)P(A2&#x200B;)&#x22EF;P(An&#x200B;)=i=1&#x220F;n&#x200B;P(Ai&#x200B;). However, it is almost always easier to deal with this kind of quantity after taking the log: log&#x2061;(&#x220F;i=1nP(Ai))=&#x2211;i=1nlog&#x2061;P(Ai).\\log \\left( \\prod_{i=1}^n P(A_i) \\right) = \\sum_{i=1}^n \\log P(A_i).log(i=1&#x220F;n&#x200B;P(Ai&#x200B;))=i=1&#x2211;n&#x200B;logP(Ai&#x200B;). To see why, go ahead and multiply a bunch of probabilities together and see how useful the result is to work with. The same trick can be used with dependent terms as well, although the results are messier. It is hard to overstate how often you will do this. This isn&#x2019;t some occasional trick &#x2013; this is standard operating procedure, so it is critical that you know the properties of exponents and logs extremely well. "],["functions.html", "2.1 Functions", " 2.1 Functions The concept of a function is not difficult or foreign, but since it&#x2019;s the most important concept in all of mathematics, it&#x2019;s worth reviewing and knowing the formal definition. Definition: Given two sets, AAA and BBB, a function (or map) is a rule that assigns, to each element in AAA, exactly one element from BBB. The set AAA is called the domain of the function and the set BBB is called the range. This is represented by the mathematical notation f:A&#x21A6;Bf: A \\mapsto Bf:A&#x21A6;B. Commentary A few remarks on this definition and its implications: This is an extremely general definition. AAA and BBB could be sets of numbers, but not necessarily: AAA could be a set of numbers and BBB could consist of intervals, with each interval being itself an infinite collection of numbers. Or AAA and BBB might not involve numbers at all. They can be anything. The only restriction is that given the same input x&#x2208;Ax \\in Ax&#x2208;A, we always get the same output f(x)&#x2208;Bf(x) \\in Bf(x)&#x2208;B. Sometimes domains are obvious from context and not explicitly specified, but it&#x2019;s an important part of the function. For example, consider the function f(x)=xf(x) = \\sqrt{x}f(x)=x&#x200B;. This is not a function that works for all numbers &#x2013; in particular, it doesn&#x2019;t work for negative numbers. The domain, then, is the set of non-negative numbers {x:x&#x2265;0}\\{x:x \\ge 0\\}{x:x&#x2265;0}. Functions don&#x2019;t have to be defined everywhere, they just need to work on their domain1. Keep in mind that a function needs to be defined for every element in its domain. This can get complicated, especially if your function is the integral of another function (as probability functions are). It&#x2019;s tempting to say, &#x201C;The domain of my function is &#x2018;any set of numbers&#x2019;. You enter a set, it returns a value.&#x201D; However, this is dangerous &#x2013; a devious troublemaker could say, &#x201C;Oh? How about the set of transcendental numbers?&#x201D; Do you really want to be responsible for defining the value of your function for such complicated sets? Sometimes you need to limit the domain to make defining the function easier. Keep this in mind when you encounter things like &#x201C;sigma algebras&#x201D;, typically one of the most bewildering concepts to grasp for first-year students. If this seems very abstract, don&#x2019;t worry too much about it &#x2013; for the purposes of this review, domain and range will almost always be sets of single numbers, but it&#x2019;s worth keeping an open mind about what functions can represent, since at various points in your education you may encounter other kinds of functions, especially functions that map vectors or matrices to numbers (or to other vectors or matrices). Inverse functions Recall that for a given input x&#x2208;Ax \\in Ax&#x2208;A, the function must always return the exact same element of BBB. The converse, however, is not true: there may be lots of elements of AAA that all get mapped to the same element of BBB. For example, in statistics one often encounters &#x201C;indicator functions&#x201D; that can have various types of things as input but always return a 0 or 1 as output (i.e., the range of an indicator function is the set {0,1}\\{0,1\\}{0,1}). Now, if it is the case that whenever x1&#x2260;x2x_1 \\ne x_2x1&#x200B;&#xE020;=x2&#x200B;, we have f(x1)&#x2260;f(x2)f(x_1) \\ne f(x_2)f(x1&#x200B;)&#xE020;=f(x2&#x200B;), then this is a special class of function called a 1:1 function. Such functions are important because they have inverses: there exists a function f&#x2212;1f^{-1}f&#x2212;1 such that whenever f(a)=bf(a)=bf(a)=b, we have f&#x2212;1(b)=af^{-1}(b)=af&#x2212;1(b)=a. A function has an inverse if and only if it is 1:1. This is important to be aware of, since there are a number of important results involving inverses, but be aware that not all functions have inverses. For example, f(x)=x2f(x)=x^2f(x)=x2 does not have an inverse: f&#x2212;1(4)f^{-1}(4)f&#x2212;1(4) could be either 2 or -22. We could extend the domain of the function to include negative numbers, but then the range would have to include complex numbers.&#x21A9;&#xFE0E; The astute reader may note that f(x)=x2f(x)=x^2f(x)=x2 could be 1:1 if I restrict its domain to be, say, (0,&#x221E;)(0,\\infty)(0,&#x221E;).&#x21A9;&#xFE0E; "],["limits-and-continuity.html", "2.2 Limits and continuity", " 2.2 Limits and continuity Limits Definition: We say that the limit of a function f(x)f(x)f(x), as xxx approaches aaa, is LLL if we can make the values of f(x)f(x)f(x) get as close as we want to LLL by taking xxx sufficiently close to aaa (but not equal to a)3. Mathematically, we can express this idea as lim&#x2061;x&#x2192;af(x)=L.\\lim_{x \\to a} f(x) = L.x&#x2192;alim&#x200B;f(x)=L. For example, if f(x)=x2f(x)=x^2f(x)=x2, then it is the case that lim&#x2061;x&#x2192;5f(x)=5.\\lim_{x \\to \\sqrt{5}} f(x) = 5.x&#x2192;5&#x200B;lim&#x200B;f(x)=5. Suppose we set xxx equal to 2.236 (this is close to 5\\sqrt{5}5&#x200B; but not equal). Then f(x)=f(x)=f(x)= 4.999696, which is close to 5. There is no value of xxx other than 5\\sqrt{5}5&#x200B; such that f(x)=5f(x)=5f(x)=5, but we can get as close as we want by moving xxx closer to 5\\sqrt{5}5&#x200B;. For example, if 4.999696 isn&#x2019;t close enough to satisfy us and someone demands that we be within 0.000000001 of 5, we can always accomplish that by simply moving xxx closer to 5\\sqrt{5}5&#x200B;. Infinite limit: A variation on this idea is to say that the limit is infinite: lim&#x2061;x&#x2192;af(x)=&#x221E;.\\lim_{x \\to a} f(x) = \\infty.x&#x2192;alim&#x200B;f(x)=&#x221E;. This means that as xxx gets closer to aaa, f(x)f(x)f(x) keeps getting bigger, with no bound. For example, we can make 1/x21/x^21/x2 be as large as we want by moving xxx closer to 0, so lim&#x2061;x&#x2192;01/x2=&#x221E;\\lim_{x \\to 0} 1/x^2 = \\inftylimx&#x2192;0&#x200B;1/x2=&#x221E; (limits of &#x2212;&#x221E;-\\infty&#x2212;&#x221E; are defined similarly). One-sided limit: Sometimes, different things happen if we approach aaa from the left or right. We say that the left-hand limit of f(x)f(x)f(x) as xxx approaches aaa &#x201C;from the left&#x201D; is LLL if f(x)f(x)f(x) we can make the values of f(x)f(x)f(x) as close to LLL as we want by moving xxx closer to aaa, but only considering points such that x&lt;ax &lt; ax&lt;a. We denote this by lim&#x2061;x&#x2192;a&#x2212;f(x)=L.\\lim_{x \\to a^-} f(x) = L.x&#x2192;a&#x2212;lim&#x200B;f(x)=L. Right-hand limits are defined similarly. For example lim&#x2061;x&#x2192;0&#x2212;1/x=&#x2212;&#x221E;\\lim_{x \\to 0^-} 1/x = -\\inftylimx&#x2192;0&#x2212;&#x200B;1/x=&#x2212;&#x221E;, whereas lim&#x2061;x&#x2192;0+1/x=&#x221E;\\lim_{x \\to 0^+} 1/x = \\inftylimx&#x2192;0+&#x200B;1/x=&#x221E;. The limit of f(x)f(x)f(x) as x&#x2192;ax \\to ax&#x2192;a is LLL if and only both the left and the right-hand limits are also LLL. Calculating limits Limit laws: The following laws are helpful for calculating limits. In what follows, let s=lim&#x2061;x&#x2192;af(x)t=lim&#x2061;x&#x2192;ag(x);\\begin{align*} s &amp;= \\lim_{x \\to a} f(x) \\\\ t &amp;= \\lim_{x \\to a} g(x); \\end{align*}st&#x200B;=x&#x2192;alim&#x200B;f(x)=x&#x2192;alim&#x200B;g(x);&#x200B; it is critical that these limits exist, or none of the results below necessarily hold. lim&#x2061;x&#x2192;a{f(x)+g(x)}=s+tlim&#x2061;x&#x2192;a{f(x)&#x2212;g(x)}=s&#x2212;tlim&#x2061;x&#x2192;a{cf(x)+g(x)}=cs+t&#xA0;where&#xA0;c&#xA0;is&#xA0;a&#xA0;constantlim&#x2061;x&#x2192;a{f(x)g(x)}=stlim&#x2061;x&#x2192;af(x)g(x)=st&#xA0;if&#xA0;t&#x2260;0lim&#x2061;x&#x2192;a{f(x)n}=sn\\begin{align*} \\lim_{x \\to a} \\{f(x) + g(x)\\} &amp;= s + t \\\\ \\lim_{x \\to a} \\{f(x) - g(x)\\} &amp;= s - t \\\\ \\lim_{x \\to a} \\{cf(x) + g(x)\\} &amp;= cs + t \\text{ where $c$ is a constant} \\\\ \\lim_{x \\to a} \\{f(x) g(x)\\} &amp;= st \\\\ \\lim_{x \\to a} \\frac{f(x)}{g(x)} &amp;= \\frac{s}{t} \\text{ if } t \\ne 0 \\\\ \\lim_{x \\to a} \\{f(x)^n\\} &amp;= s^n \\end{align*}x&#x2192;alim&#x200B;{f(x)+g(x)}x&#x2192;alim&#x200B;{f(x)&#x2212;g(x)}x&#x2192;alim&#x200B;{cf(x)+g(x)}x&#x2192;alim&#x200B;{f(x)g(x)}x&#x2192;alim&#x200B;g(x)f(x)&#x200B;x&#x2192;alim&#x200B;{f(x)n}&#x200B;=s+t=s&#x2212;t=cs+t&#xA0;where&#xA0;c&#xA0;is&#xA0;a&#xA0;constant=st=ts&#x200B;&#xA0;if&#xA0;t&#xE020;=0=sn&#x200B; Continuity You may have noticed that with limits, the value of f(x)f(x)f(x) at aaa is irrelevant. For example, if f(x)=x2f(x)=x^2f(x)=x2 everywhere except x=2x=2x=2, where f(2)=&#x2212;10f(2) = -10f(2)=&#x2212;10, it would still be the case that lim&#x2061;x&#x2192;2f(x)=4\\lim_{x \\to 2} f(x) = 4limx&#x2192;2&#x200B;f(x)=4. In fact, f(x)f(x)f(x) wouldn&#x2019;t even need to be defined at 2 for this to work. If we add the requirement that f(a)f(a)f(a) has to equal its limit, we end up with continuity. Definition: A function fff is continuous at aaa if lim&#x2061;x&#x2192;af(x)=f(a).\\lim_{x \\to a} f(x) = f(a).x&#x2192;alim&#x200B;f(x)=f(a). Note that this requires three things: f(a)f(a)f(a) is defined lim&#x2061;x&#x2192;af(x)\\lim_{x \\to a} f(x)limx&#x2192;a&#x200B;f(x) exists These two things are equal Expanding on this definition, we say that a function fff is continuous on an interval if fff is continuous at every number in the interval. We say that fff is continuous if fff is continuous at every point in its domain. One-sided continuity: A function fff is continuous from the left at aaa if lim&#x2061;x&#x2192;a&#x2212;f(x)=f(a).\\lim_{x \\to a^-} f(x) = f(a).x&#x2192;a&#x2212;lim&#x200B;f(x)=f(a). For example, consider the function f(x)={0&#xA0;if&#xA0;x&lt;01&#xA0;if&#xA0;x&#x2265;0f(x) = \\begin{cases} 0 &amp; \\text{ if } x &lt; 0 \\\\ 1 &amp; \\text{ if } x \\ge 0 \\end{cases}f(x)={01&#x200B;&#xA0;if&#xA0;x&lt;0&#xA0;if&#xA0;x&#x2265;0&#x200B; In this case, f(x)f(x)f(x) is continuous from the right at 0, but not from the left at 0 (since lim&#x2061;x&#x2192;0&#x2212;=0\\lim_{x \\to 0^-}=0limx&#x2192;0&#x2212;&#x200B;=0, but f(0)=1f(0) = 1f(0)=1). Continuity laws: The property of continuity behaves similarly to the limit laws above. If f(x)f(x)f(x) and g(x)g(x)g(x) are continuous at aaa, then the following functions are also continuous at aaa: f(x)+g(x)f(x) + g(x)f(x)+g(x) f(x)&#x2212;g(x)f(x) - g(x)f(x)&#x2212;g(x) cf(x)c f(x)cf(x), where ccc is a constant f(x)g(x)f(x) g(x)f(x)g(x) f(x)/g(x)f(x) / g(x)f(x)/g(x) if g(a)&#x2260;0g(a) \\ne 0g(a)&#xE020;=0 Composition: Finally, suppose that ggg is continuous at aaa and that fff is continuous at g(a)g(a)g(a). Then f(g(x))f(g(x))f(g(x)) is continuous at aaa. In words, a continuous function of a continuous function is continuous. The function h(x)=f(g(x))h(x) = f(g(x))h(x)=f(g(x)) is known as the composition of fff and ggg. This section covers limits and continuity from a conceptual standpoint. For a variety of technical reasons, the definition given here isn&#x2019;t actually satisfactory, and a more rigorous definition is required; see the chapter on analysis.&#x21A9;&#xFE0E; "],["derivatives.html", "2.3 Derivatives", " 2.3 Derivatives Definition The slope of a straight line is straightforward: &#x394;y/&#x394;x\\Delta y / \\Delta x&#x394;y/&#x394;x. For a curved line, however, we will get different answers depending on the range over which we calculate these changes. Nevertheless, we can calculate the limit of this slope over shorter and shorter ranges. This is known as the derivative of the function. Definition: The derivative of a function fff at aaa, denoted f&#x2032;(a)f&apos;(a)f&#x2032;(a), is f&#x2032;(a)=lim&#x2061;h&#x2192;0f(a+h)&#x2212;f(a)hf&apos;(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}f&#x2032;(a)=h&#x2192;0lim&#x200B;hf(a+h)&#x2212;f(a)&#x200B; if this limit exists. If the limit exists, fff is said to be differentiable at aaa. If a function is not continuous at aaa, it is not possible for it to be differentiable at aaa. The converse, however, is not true. For example, the function f(x)=&#x2223;x&#x2223;f(x) = \\lvert x\\rvertf(x)=&#x2223;x&#x2223; is continuous everywhere, and differentiable just about everywhere, but not differentiable at zero since the limit from the left is -1 and the limit from the right is 1. Expanding on this pointwise definition, we can define a whole function, f&#x2032;(x)f&apos;(x)f&#x2032;(x). This function is known as the derivative of fff. Formulas It is hard to overstate the importance of knowing the following formulas; you will use them constantly. Below, we assume that fff and ggg are differentiable, and that ccc is a constant. c&#x2032;=0(xn)&#x2032;=nxn&#x2212;1(cf)&#x2032;=cf&#x2032;(f+g)&#x2032;=f&#x2032;+g&#x2032;(f&#x2212;g)&#x2032;=f&#x2032;&#x2212;g&#x2032;(fg)&#x2032;=fg&#x2032;+gf&#x2032;&#xA0;(product&#xA0;rule)(fg)&#x2032;=gf&#x2032;&#x2212;fg&#x2032;g2&#xA0;(quotient&#xA0;rule)\\begin{align*} c&apos; &amp;= 0 \\\\ (x^n)&apos; &amp;= n x^{n-1} \\\\ (cf)&apos; &amp;= c f&apos; \\\\ (f + g)&apos; &amp;= f&apos; + g&apos; \\\\ (f - g)&apos; &amp;= f&apos; - g&apos; \\\\ (fg)&apos; &amp;= fg&apos; + gf&apos; \\text{ (product rule)}\\\\ \\left( \\frac{f}{g} \\right)&apos; &amp;= \\frac{gf&apos; - fg&apos;}{g^2} \\text{ (quotient rule)}\\\\ \\end{align*}c&#x2032;(xn)&#x2032;(cf)&#x2032;(f+g)&#x2032;(f&#x2212;g)&#x2032;(fg)&#x2032;(gf&#x200B;)&#x2032;&#x200B;=0=nxn&#x2212;1=cf&#x2032;=f&#x2032;+g&#x2032;=f&#x2032;&#x2212;g&#x2032;=fg&#x2032;+gf&#x2032;&#xA0;(product&#xA0;rule)=g2gf&#x2032;&#x2212;fg&#x2032;&#x200B;&#xA0;(quotient&#xA0;rule)&#x200B; These basic rules can be combined into all sorts of additional rules with the chain rule, which states that if the derivatives g&#x2032;(x)g&apos;(x)g&#x2032;(x) and f&#x2032;(g(x))f&apos;(g(x))f&#x2032;(g(x)) both exist, then the derivative of f(g(x))f(g(x))f(g(x)) exists, and its derivative is f&#x2032;(g(x))g&#x2032;(x)f&apos;(g(x))g&apos;(x)f&#x2032;(g(x))g&#x2032;(x). The rule is often expressed in Leibniz notation: dfdx=dfdgdgdx.\\frac{df}{dx} = \\frac{df}{dg} \\frac{dg}{dx}.dxdf&#x200B;=dgdf&#x200B;dxdg&#x200B;. The section on logarthim and exponential functions provides additional important differentiation formulas. Higher derivatives Since f&#x2032;(x)f&apos;(x)f&#x2032;(x) is itself a function, we can also take its derivative. This is called the second derivative of fff, and is denoted f&#x2032;&#x2032;(x)f&apos;&apos;(x)f&#x2032;&#x2032;(x). Third derivatives, fourth derivatives, and so on are defined similarly. An important application of higher derivatives is to approximate functions. The linear approximation of fff at aaa is given by f(x)&#x2248;f(a)+f&#x2032;(a)(x&#x2212;a).f(x) \\approx f(a) + f&apos;(a) (x - a).f(x)&#x2248;f(a)+f&#x2032;(a)(x&#x2212;a). The quadratic approximation of fff at aaa is given by f(x)&#x2248;f(a)+f&#x2032;(a)(x&#x2212;a)+12f&#x2032;&#x2032;(a)(x&#x2212;a)2.f(x) \\approx f(a) + f&apos;(a) (x - a) + \\frac{1}{2}f&apos;&apos;(a) (x - a)^2.f(x)&#x2248;f(a)+f&#x2032;(a)(x&#x2212;a)+21&#x200B;f&#x2032;&#x2032;(a)(x&#x2212;a)2. To see these approximations in action, here&#x2019;s a figure: Note that (a) both approximations are very good close to aaa, which is denoted by the black dot and (b) the quadratic approximation is more accurate than the linear approximation. Both of these observations are true broadly speaking; they are not particular to this example. "],["optimization.html", "2.4 Optimization", " 2.4 Optimization Terminology The most useful thing about derivatives is that they enable us to find the maximum and minimum values of a function. As noted earlier, this arises constantly in statistics. First, some terminology (below, fff is a function with domain DDD): absolute maximum: The point ccc is an absolute maximum of fff if f(c)&#x2265;f(x)f(c) \\ge f(x)f(c)&#x2265;f(x) for all xxx in DDD. maximum value: The maximum value of fff is f(c)f(c)f(c) , where ccc is an absolute maximum of fff. local maximum: The point ccc is a local maximum (or relative maximum) of fff if there is an interval III containing ccc such that f(c)&#x2265;f(x)f(c) \\ge f(x)f(c)&#x2265;f(x) for all xxx in III. Absolute minimum, minimum value, and local minimum are defined similarly. Finally, a point ccc is an extreme value if ccc is either an absolute maximum or an absolute minimum, while ccc is a local extremum if ccc is a local maximum or local minimum. Derivatives and extreme values What does this have to do with derivatives? The following result is so important, you should memorize it word for word and never forget it. If fff has a local extremum at ccc, and if f&#x2032;(c)f&apos;(c)f&#x2032;(c) exists, then f&#x2032;(c)=0f&apos;(c) = 0f&#x2032;(c)=0. A point ccc satisfying f&#x2032;(c)=0f&apos;(c) = 0f&#x2032;(c)=0 is called a critical point of fff. Practically speaking, this means that if we want to maximize or minimize a function, we just need to find its critical points. However, we do need to be aware of a few caveats: The derivative has to exist. For example, we cannot minimize f(x)=&#x2223;x&#x2223;f(x) = \\lvert x\\rvertf(x)=&#x2223;x&#x2223; with derivatives, because the minimum occurs at 0 and fff is not differentiable at 0. The converse of the above statement is not true. It is true that if ccc is a local extremum (and fff differentiable), then ccc is a critical point. However, ccc can be a critical point without being an extremum. For example, 0 is a critical point of f(x)=x3f(x) = x^3f(x)=x3, but it is not a local minimum or maximum. If we find a critical point ccc, even if it is an extremum, we don&#x2019;t know whether it minimizes or maximizes fff. The function fff might not have any critical points. More information about caveats 3 and 4 is given below. Don&#x2019;t led these caveats obscure the main result, though &#x2013; this is arguably the most useful thing in all of calculus. Monotonicity and convexity Monotone functions: If fff is differentiable, why wouldn&#x2019;t it have any critical points (#4 above)? The most likely answer is that it is monotone. A function fff is called increasing if f(x1)&lt;f(x2)f(x_1) &lt; f(x_2)f(x1&#x200B;)&lt;f(x2&#x200B;) for all x1&lt;x2x_1 &lt; x_2x1&#x200B;&lt;x2&#x200B; and decreasing if f(x1)&gt;f(x2)f(x_1) &gt; f(x_2)f(x1&#x200B;)&gt;f(x2&#x200B;) for all x1&lt;x2x_1 &lt; x_2x1&#x200B;&lt;x2&#x200B;. A function that is either increasing or decreasing is called monotone. For a differentiable function, whether it is monotone or not is related to its derivative: If f&#x2032;(x)&gt;0f&apos;(x) &gt; 0f&#x2032;(x)&gt;0 for all xxx, then fff is increasing. If f&#x2032;(x)&lt;0f&apos;(x) &lt; 0f&#x2032;(x)&lt;0 for all xxx, then fff is decreasing. So there you have it. If fff is differentiable, there are three possibilities: it is always going up (f&#x2032;(x)&gt;0f&apos;(x) &gt; 0f&#x2032;(x)&gt;0), always going down (f&#x2032;(x)&lt;0f&apos;(x) &lt; 0f&#x2032;(x)&lt;0), or sometimes going up and sometimes going down, in which case it will cross zero and have a critical point (due to the intermediate value theorem.) Tests for min/max: Often, it is obvious whether a critical point ccc is a minimum or maximum. However, if you&#x2019;re not sure, you can do one of two things: Plug in a number less than ccc, then greater than ccc. If f&#x2032;f&apos;f&#x2032; changes from negative to positive, ccc is a local minimum. If it changes from positive to negative, ccc is a local maximum. If it does not change sign, ccc is not a local extremum. This is known as the &#x201C;first derivative test&#x201D;. Take the second derivative at ccc (assuming it exists). If f&#x2032;&#x2032;(c)&gt;0f&apos;&apos;(c) &gt; 0f&#x2032;&#x2032;(c)&gt;0, then ccc is a local minimum. If f&#x2032;&#x2032;(c)&lt;0f&apos;&apos;(c) &lt; 0f&#x2032;&#x2032;(c)&lt;0, then ccc is a local maximum. This is known as the &#x201C;second derivative test&#x201D;. Note that if f&#x2032;&#x2032;(c)=0f&apos;&apos;(c) = 0f&#x2032;&#x2032;(c)=0, the test is inconclusive &#x2013; ccc could be a local max, a local min, or neither. Convexity and concavity: If a function is always curving upwards or downwards, then no tests are needed and no distinctions between local and global extrema are necessary. To define this formally, imagine drawing a tangent line to a function fff at every point in its domain. If fff always lies above the tangent line, it is said to be convex (curving upwards). If fff always lies below the tangent line, it is concave (curving downwards). With respect to optimization, If fff is convex, then any critical point is a global minimum. If fff is concave, then any critical point is a global maximum. Some textbooks / math classes refer to these as &#x201C;concave up&#x201D; and &#x201C;concave down&#x201D;, but you should learn concave/convex since it is far more common in the statistics, mathematics, and optimization literature. Optimization is an enormous subject with giant textbooks devoted to it, so obviously this isn&#x2019;t the whole story. However, taking the derivative and setting it equal to zero truly is the main idea, and solves a huge range of optimization problems. "],["integration.html", "2.5 Integration", " 2.5 Integration There are two core questions with which calculus is concerned. One is generalizing the idea of slope to nonlinear functions. The other is how to calculate the total contribution of some entity, where the contribution at any given instant is given by a function. As with slopes, this is trivial if the function is linear and becomes much harder when the function is nonlinear. For example, if someone burns 700 calories/hr while exercising, and they exercise for half an hour, then they burn 350 calories. But what if their exercise intensity varies over time, with f(t)f(t)f(t) describing the rate at time ttt (in minutes)? In this case we would have to add the contributions: f(0)160+f(1)160+&#x2026;.f(0)\\frac{1}{60} + f(1)\\frac{1}{60} + \\ldots.f(0)601&#x200B;+f(1)601&#x200B;+&#x2026;. However, this still doesn&#x2019;t really answer the question, as it assumes f(t)f(t)f(t) is constant over the first minute, then allowed to change, then constant again for the next minute, and so on. We could get a more accurate answer by summing up these contributions at each second, and still more accurate by summing over each nanosecond, and so on. The limit of this process is known as the &#x201C;integral&#x201D;, which we define below. As noted earlier, this comes up constantly in statistics when calculating probabilities and expected values. Definition Let the interval [a,b][a, b][a,b] be partitioned as follows: a=x0&lt;x1&lt;x2&lt;&#x22EF;&lt;xn=b,a = x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_n = b,a=x0&#x200B;&lt;x1&#x200B;&lt;x2&#x200B;&lt;&#x22EF;&lt;xn&#x200B;=b, let xi&#x2217;x_i^*xi&#x2217;&#x200B; be any point in [xi&#x2212;1,xi][x_{i-1}, x_i][xi&#x2212;1&#x200B;,xi&#x200B;], &#x394;xi=xi&#x2212;xi&#x2212;1\\Delta x_i = x_i - x_{i-1}&#x394;xi&#x200B;=xi&#x200B;&#x2212;xi&#x2212;1&#x200B;, and m=max&#x2061;{&#x394;x1,&#x394;x2,&#x2026;,&#x394;xn}m = \\max\\{\\Delta x_1, \\Delta x_2, \\ldots, \\Delta x_n\\}m=max{&#x394;x1&#x200B;,&#x394;x2&#x200B;,&#x2026;,&#x394;xn&#x200B;}. Then the integral of fff from aaa to bbb is &#x222B;abf(x)&#x2009;dx=lim&#x2061;m&#x2192;0&#x2211;i=1nf(xi&#x2217;)&#x394;xi\\int_a^b f(x) \\,dx= \\lim_{m \\to 0} \\sum_{i=1}^n f(x_i^*) \\Delta x_i&#x222B;ab&#x200B;f(x)dx=m&#x2192;0lim&#x200B;i=1&#x2211;n&#x200B;f(xi&#x2217;&#x200B;)&#x394;xi&#x200B; if this limit exists. If the limit does exist, then fff is said to be integrable over [a,b][a, b][a,b]. Relating this definition to our example above, mmm represents the time resolution and lim&#x2061;m&#x2192;0\\lim_{m \\to 0}limm&#x2192;0&#x200B; represents moving from minutes to seconds to nanoseconds and so on. The above definition assumes that a&lt;ba &lt; ba&lt;b; if a&gt;ba &gt; ba&gt;b the integral is defined as &#x222B;abf(x)&#x2009;dx=&#x2212;&#x222B;baf(x)&#x2009;dx.\\int_a^b f(x) \\,dx= -\\int_b^a f(x) \\,dx.&#x222B;ab&#x200B;f(x)dx=&#x2212;&#x222B;ba&#x200B;f(x)dx. How can we know if a function is integrable? If fff is either continuous or monotonic on [a,b][a, b][a,b], then fff is integrable on [a,b][a, b][a,b]. If fff is jumping up and down discontinuously, then anything can happen &#x2013; fff may or may not be integrable, and we would need a deep dive into the theory of integration to really answer this question. Thankfully, as a first-year graduate student, you will only ever need to integrate continuous functions. Properties of integrals If all of the following integrals exist, then they obey these rules: &#x222B;abc&#x2009;dx=c(b&#x2212;a)&#x222B;abcf(x)&#x2009;dx=c&#x222B;abf(x)&#x2009;dx&#x222B;ab{f(x)+g(x)}&#x2009;dx=&#x222B;abf(x)&#x2009;dx+&#x222B;abg(x)&#x2009;dx&#x222B;ab{f(x)&#x2212;g(x)}&#x2009;dx=&#x222B;abf(x)&#x2009;dx&#x2212;&#x222B;abg(x)&#x2009;dx&#x222B;abf(x)&#x2009;dx=&#x222B;acf(x)&#x2009;dx+&#x222B;cbf(x)&#x2009;dx\\begin{align*} \\int_a^b c \\,dx&amp;= c(b-a) \\\\ \\int_a^b c f(x) \\,dx&amp;= c \\int_a^b f(x) \\,dx\\\\ \\int_a^b \\bigl\\{f(x) + g(x)\\bigr\\} \\,dx&amp;= \\int_a^b f(x) \\,dx+ \\int_a^b g(x) \\,dx\\\\ \\int_a^b \\bigl\\{f(x) - g(x)\\bigr\\} \\,dx&amp;= \\int_a^b f(x) \\,dx- \\int_a^b g(x) \\,dx\\\\ \\int_a^b f(x) \\,dx&amp;= \\int_a^c f(x) \\,dx+ \\int_c^b f(x) \\,dx \\end{align*}&#x222B;ab&#x200B;cdx&#x222B;ab&#x200B;cf(x)dx&#x222B;ab&#x200B;{f(x)+g(x)}dx&#x222B;ab&#x200B;{f(x)&#x2212;g(x)}dx&#x222B;ab&#x200B;f(x)dx&#x200B;=c(b&#x2212;a)=c&#x222B;ab&#x200B;f(x)dx=&#x222B;ab&#x200B;f(x)dx+&#x222B;ab&#x200B;g(x)dx=&#x222B;ab&#x200B;f(x)dx&#x2212;&#x222B;ab&#x200B;g(x)dx=&#x222B;ac&#x200B;f(x)dx+&#x222B;cb&#x200B;f(x)dx&#x200B; If we further suppose that a&lt;ba &lt; ba&lt;b, then we also have If f(x)&#x2265;0f(x) \\ge 0f(x)&#x2265;0 for all xxx, then &#x222B;abf(x)&#x2009;dx&#x2265;0\\int_a^b f(x) \\,dx\\ge 0&#x222B;ab&#x200B;f(x)dx&#x2265;0. If f(x)&#x2265;g(x)f(x) \\ge g(x)f(x)&#x2265;g(x) for all xxx, then &#x222B;abf(x)&#x2009;dx&#x2265;&#x222B;abg(x)&#x2009;dx\\int_a^b f(x) \\,dx\\ge \\int_a^b g(x) \\,dx&#x222B;ab&#x200B;f(x)dx&#x2265;&#x222B;ab&#x200B;g(x)dx. If m&#x2264;f(x)&#x2264;Mm \\le f(x) \\le Mm&#x2264;f(x)&#x2264;M for all xxx, then m(b&#x2212;a)&#x2264;&#x222B;abf(x)&#x2009;dx&#x2264;M(b&#x2212;a)m(b-a) \\le \\int_a^b f(x) \\,dx\\le M(b-a)m(b&#x2212;a)&#x2264;&#x222B;ab&#x200B;f(x)dx&#x2264;M(b&#x2212;a) &#x2223;&#x222B;abf(x)&#x2009;dx&#x2223;&#x2264;&#x222B;ab&#x2223;f(x)&#x2223;&#x2009;dx\\Bigl\\lvert \\int_a^b f(x) \\,dx\\Bigr\\rvert \\le \\int_a^b \\bigl\\lvert f(x) \\bigr\\rvert \\,dx&#x200B;&#x222B;ab&#x200B;f(x)dx&#x200B;&#x2264;&#x222B;ab&#x200B;&#x200B;f(x)&#x200B;dx Finally, if a=ba = ba=b, then &#x222B;abf(x)&#x2009;dx=0.\\int_a^b f(x) \\,dx= 0.&#x222B;ab&#x200B;f(x)dx=0. Fundamental theorem of calculus Somewhat remarkably, the two branches of calculus (differentiation and integration) are closely related. This relationship is known as the fundamental theorem of calculus: If fff is continuous on [a,b][a, b][a,b], then g(x)=&#x222B;axf(t)&#x2009;dtg(x) = \\int_a^x f(t) \\,dtg(x)=&#x222B;ax&#x200B;f(t)dt is continuous and differentiable and g&#x2032;(x)=f(x)g&apos;(x) = f(x)g&#x2032;(x)=f(x). In other words, if we integrate a function, then differentiate the result, we get back to the original function. The same is true if we start with differentiation: If fff is continuous on [a,b][a, b][a,b], then &#x222B;abf(x)&#x2009;dx=F(b)&#x2212;F(a)\\int_a^b f(x) \\,dx= F(b) - F(a)&#x222B;ab&#x200B;f(x)dx=F(b)&#x2212;F(a) where FFF is any function that satisfies F&#x2032;=fF&apos; = fF&#x2032;=f. Functions satisfying F&#x2032;=fF&apos; = fF&#x2032;=f are particularly important, and discussed below. Antiderivatives A function FFF is called an antiderivative (or a primitive) of fff if F&#x2032;(x)=f(x)F&apos;(x) = f(x)F&#x2032;(x)=f(x) for all xxx. This can also be written &#x222B;f(x)&#x2009;dx=F(x).\\int f(x) \\,dx= F(x).&#x222B;f(x)dx=F(x). This &#x201C;equation&#x201D; means the same thing, that F&#x2032;(x)=f(x)F&apos;(x) = f(x)F&#x2032;(x)=f(x) for all xxx. However, it is not truly an equation since there are an infinite number of functions FFF that satisfy F&#x2032;(x)=f(x)F&apos;(x) = f(x)F&#x2032;(x)=f(x) for all xxx. For example, both of the following are correct: &#x222B;2x&#x2009;dx=x2&#x222B;2x&#x2009;dx=x2+5.\\begin{align*} \\int 2x \\,dx&amp;= x^2 \\\\ \\int 2x \\,dx&amp;= x^2 + 5. \\\\ \\end{align*}&#x222B;2xdx&#x222B;2xdx&#x200B;=x2=x2+5.&#x200B; This is potentially confusing because the left hand side is the same in each case, but the right hand side is different &#x2013; hence the scare quotes around &#x201C;equation&#x201D;. Some people prefer to write &#x222B;2x&#x2009;dx=x2+C\\int 2x \\,dx= x^2 + C&#x222B;2xdx=x2+C to emphasize this point. Whether you do this or not is up to you, but either way, it is critical to understand the distinction between &#x222B;abf(x)&#x2009;dx\\int_a^b f(x) \\,dx&#x222B;ab&#x200B;f(x)dx and &#x222B;f(x)&#x2009;dx\\int f(x) \\,dx&#x222B;f(x)dx. The first quantity (with the integration limits) is a known as a definite integral, and it is a number. The second quantity (without the limits) is known as an indefinite integral, and it is not a number &#x2013; it is a function (or more precisely, a collection of an infinite number of functions)4. So what&#x2019;s the point of antiderivatives/primitives/indefinite integrals? If we have one, we can easily calculate (definite) integrals. For example, &#x222B;142x&#x2009;dx=F(4)&#x2212;F(1)=42&#x2212;12=15.\\begin{align*} \\int_1^4 2x \\,dx&amp;= F(4) - F(1) \\\\ &amp;= 4^2 - 1^2 \\\\ &amp;= 15. \\end{align*}&#x222B;14&#x200B;2xdx&#x200B;=F(4)&#x2212;F(1)=42&#x2212;12=15.&#x200B; Note that I get the same answer regardless of which antiderivative I use (i.e., it&#x2019;s not important to find the collection of all antiderivates&#x2026;any antiderivative is fine). In other words, we can integrate any function fff if can find an antiderivative of it. How do we find these antiderivatives? Unfortunately, this is often challenging and sometimes impossible. However, there are several techniques for doing this, which will be discussed in a later section. Students sometimes have a tendency to think of the indefinite integral as the &#x201C;true&#x201D; integral and the definite integral as an application of it. This is completely backwards. The indefinite integral is actually a statement about derivatives. We don&#x2019;t even need to define the concept of an integral in order to say that &#x222B;2x&#x2009;dx=x2\\int 2x \\,dx= x^2&#x222B;2xdx=x2. It is only once the integral has been defined and the fundamental theorem of calculus has been proven that indefinite integrals have any purpose. The &#x201C;definite&#x201D; integral defines the concept of the integral; the only reason we add the &#x201C;definite&#x201D; modifier to distinguish them from indefinite integrals.&#x21A9;&#xFE0E; "],["logarithm-and-exponential.html", "2.6 Logarithm and exponential", " 2.6 Logarithm and exponential Exponential definition The exponential function is f(x)=axf(x) = a^xf(x)=ax; the base aaa must be a positive real number but the exponent xxx can be any real number. The precise definition doesn&#x2019;t come up often, but here it is in case you ever need it (defining all these cases is necessary in order to ensure that the resulting function is continuous: If xxx is a positive integer nnn, then an=a&#x22C5;a&#x22EF;a(na^n = a \\cdot a \\cdots a \\quad (nan=a&#x22C5;a&#x22EF;a(n times) If x=0x=0x=0, then a0=1a^0 = 1a0=1 If xxx is a negative integer, then a&#x2212;n=1ana^{-n} = \\frac{1}{a^n}a&#x2212;n=an1&#x200B; If xxx is a rational number p/qp/qp/q, with q&gt;0q&gt;0q&gt;0, then ap/q=apqa^{p/q} = \\sqrt[q]{a^p}ap/q=qap&#x200B; If xxx is an irrational number, then it&#x2019;s defined as the limit of ara^rar, where rrr is a sequence of rational numbers whose limit is xxx. Note that we would run into trouble at step 4 if we tried to allow negative bases. Exponential rules ax+y=axayax&#x2212;y=axay(ax)y=axy(ab)x=axbx\\begin{align*} a^{x+y} &amp;= a^x a^y \\\\ a^{x-y} &amp;= \\frac{a^x}{a^y} \\\\ (a^x)^y &amp;= a^{xy} \\\\ (ab)^x &amp;= a^x b^x \\end{align*}ax+yax&#x2212;y(ax)y(ab)x&#x200B;=axay=ayax&#x200B;=axy=axbx&#x200B; Exponential limits lim&#x2061;x&#x2192;&#x221E;ax=&#x221E;if&#xA0;a&gt;1lim&#x2061;x&#x2192;&#x2212;&#x221E;ax=0if&#xA0;a&gt;1lim&#x2061;x&#x2192;&#x221E;ax=0if&#xA0;0&lt;a&lt;1lim&#x2061;x&#x2192;&#x2212;&#x221E;ax=&#x221E;if&#xA0;0&lt;a&lt;1lim&#x2061;h&#x2192;0eh&#x2212;1h=1lim&#x2061;n&#x2192;&#x221E;(1+1n)n=e\\begin{align*} \\lim_{x \\to \\infty} a^x &amp;= \\infty \\quad \\text{if } a &gt; 1 \\\\ \\lim_{x \\to -\\infty} a^x &amp;= 0 \\quad \\text{if } a &gt; 1 \\\\ \\lim_{x \\to \\infty} a^x &amp;= 0 \\quad \\text{if } 0 &lt; a &lt; 1 \\\\ \\lim_{x \\to -\\infty} a^x &amp;= \\infty \\quad \\text{if } 0 &lt; a &lt; 1 \\\\ \\lim_{h \\to 0} \\frac{e^h - 1}{h} &amp;= 1 \\\\ \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n}\\right)^n &amp;= e \\end{align*}x&#x2192;&#x221E;lim&#x200B;axx&#x2192;&#x2212;&#x221E;lim&#x200B;axx&#x2192;&#x221E;lim&#x200B;axx&#x2192;&#x2212;&#x221E;lim&#x200B;axh&#x2192;0lim&#x200B;heh&#x2212;1&#x200B;n&#x2192;&#x221E;lim&#x200B;(1+n1&#x200B;)n&#x200B;=&#x221E;if&#xA0;a&gt;1=0if&#xA0;a&gt;1=0if&#xA0;0&lt;a&lt;1=&#x221E;if&#xA0;0&lt;a&lt;1=1=e&#x200B; Exponential derivatives and integrals ddxex=exddxeu=exdudx&#x222B;ex&#x2009;dx=exddxax=axlog&#x2061;(a)&#x222B;ax&#x2009;dx=axlog&#x2061;a(a&#x2260;1)\\begin{align*} \\frac{d}{dx} e^x &amp;= e^x \\\\ \\frac{d}{dx} e^u &amp;= e^x \\frac{du}{dx} \\\\ \\int e^x \\, dx &amp;= e^x \\\\ \\frac{d}{dx} a^x &amp;= a^x \\log(a) \\\\ \\int a^x \\, dx &amp;= \\frac{a^x}{\\log a} \\quad (a \\ne 1) \\end{align*}dxd&#x200B;exdxd&#x200B;eu&#x222B;exdxdxd&#x200B;ax&#x222B;axdx&#x200B;=ex=exdxdu&#x200B;=ex=axlog(a)=logaax&#x200B;(a&#xE020;=1)&#x200B; Note that the last two results use the logarithmic function, which we haven&#x2019;t actually introduced yet (see below). Logarithm definition The logarithmic function with base aaa is defined as the function satisfying log&#x2061;ax=y&#x2005;&#x200A;&#x27FA;&#x2005;&#x200A;ay=x\\log_a x = y \\iff a^y = xloga&#x200B;x=y&#x27FA;ay=x If we leave off the base, it is assumed to be base eee, the &#x201C;natural logarithm&#x201D;: log&#x2061;x=log&#x2061;ex\\log x = \\log_e xlogx=loge&#x200B;x in other words, log&#x2061;x=y&#x2005;&#x200A;&#x27FA;&#x2005;&#x200A;ey=x;\\log x = y \\iff e^y = x;logx=y&#x27FA;ey=x; the notation ln&#x2061;x\\ln xlnx can also be used for this. In some disciplines, when we leave off the base, one assumes the base is 10; statistics is not one of those disciplines. Note that log&#x2061;(ex)=xelog&#x2061;x=xlog&#x2061;e=1.\\begin{align*} \\log(e^x) &amp;= x \\\\ e^{\\log x} &amp;= x \\\\ \\log e &amp;= 1. \\end{align*}log(ex)elogxloge&#x200B;=x=x=1.&#x200B; Logarithm rules log&#x2061;a(xy)=log&#x2061;ax+log&#x2061;aylog&#x2061;axy=log&#x2061;ax&#x2212;log&#x2061;aylog&#x2061;a(xy)=ylog&#x2061;axlog&#x2061;ax=log&#x2061;xlog&#x2061;a\\begin{align*} \\log_a(xy) &amp;= \\log_a x + \\log_a y \\\\ \\log_a \\frac{x}{y} &amp;= \\log_a x - \\log_a y \\\\ \\log_a (x^y) &amp;= y \\log_a x \\\\ \\log_a x &amp;= \\frac{\\log x}{\\log a} \\end{align*}loga&#x200B;(xy)loga&#x200B;yx&#x200B;loga&#x200B;(xy)loga&#x200B;x&#x200B;=loga&#x200B;x+loga&#x200B;y=loga&#x200B;x&#x2212;loga&#x200B;y=yloga&#x200B;x=logalogx&#x200B;&#x200B; Logarithm limits If a&gt;1a &gt; 1a&gt;1, then lim&#x2061;x&#x2192;&#x221E;log&#x2061;ax=&#x221E;lim&#x2061;x&#x2192;0+log&#x2061;ax=&#x2212;&#x221E;\\begin{align*} \\lim_{x \\to \\infty} \\log_a x = \\infty \\\\ \\lim_{x \\to 0^+} \\log_a x = -\\infty \\\\ \\end{align*}x&#x2192;&#x221E;lim&#x200B;loga&#x200B;x=&#x221E;x&#x2192;0+lim&#x200B;loga&#x200B;x=&#x2212;&#x221E;&#x200B; Logarithm derivatives and integrals ddxlog&#x2061;x=x&#x2212;1ddxlog&#x2061;u=u&#x2212;1dudx&#x222B;1x&#x2009;dx=log&#x2061;&#x2223;x&#x2223;ddxlog&#x2061;ax=1xlog&#x2061;a\\begin{align*} \\frac{d}{dx} \\log x &amp;= x^{-1} \\\\ \\frac{d}{dx} \\log u &amp;= u^{-1} \\frac{du}{dx} \\\\ \\int \\frac{1}{x} \\, dx &amp;= \\log|x| \\\\ \\frac{d}{dx} \\log_a x &amp;= \\frac{1}{x \\log a} \\end{align*}dxd&#x200B;logxdxd&#x200B;logu&#x222B;x1&#x200B;dxdxd&#x200B;loga&#x200B;x&#x200B;=x&#x2212;1=u&#x2212;1dxdu&#x200B;=log&#x2223;x&#x2223;=xloga1&#x200B;&#x200B; "],["improper-integrals.html", "2.7 Improper integrals", " 2.7 Improper integrals In statistics, it is very common to encounter integrals that look like this: &#x222B;0&#x221E;f(x)&#x2009;dx.\\int_0^\\infty f(x) \\,dx.&#x222B;0&#x221E;&#x200B;f(x)dx. This expression is a little confusing because if the integration region is infinite, then our earlier definition of the integral no longer works. What the expression means is that we&#x2019;re taking the limit of the definite integrals (all of which are well-defined) as the region gets larger and larger: &#x222B;0&#x221E;f(x)&#x2009;dx=lim&#x2061;b&#x2192;&#x221E;&#x222B;abf(x)&#x2009;dx.\\int_0^\\infty f(x) \\,dx= \\lim_{b \\to \\infty} \\int_a^b f(x) \\,dx.&#x222B;0&#x221E;&#x200B;f(x)dx=b&#x2192;&#x221E;lim&#x200B;&#x222B;ab&#x200B;f(x)dx. For example, &#x222B;0be&#x2212;x&#x2009;dx=1&#x2212;e&#x2212;b;\\int_0^b e^{-x} \\,dx= 1 - e^{-b};&#x222B;0b&#x200B;e&#x2212;xdx=1&#x2212;e&#x2212;b; (if you don&#x2019;t follow this derivation, see here and here). Since lim&#x2061;b&#x2192;&#x221E;e&#x2212;b=0\\lim_{b \\to \\infty} e^{-b} = 0limb&#x2192;&#x221E;&#x200B;e&#x2212;b=0 (see here), we have &#x222B;0&#x221E;e&#x2212;x&#x2009;dx=1.\\int_0^\\infty e^{-x} \\,dx= 1.&#x222B;0&#x221E;&#x200B;e&#x2212;xdx=1. Integrals with an infinite bound (either upper or lower) are known as improper integrals. There are two other kinds of improper integrals. Both bounds are infinite: One might expect that &#x222B;&#x2212;&#x221E;&#x221E;f(x)&#x2009;dx\\int_{-\\infty}^\\infty f(x) \\,dx&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;f(x)dx would be defined as the limit of &#x222B;&#x2212;aaf(x)&#x2009;dx\\int_{-a}^a f(x) \\,dx&#x222B;&#x2212;aa&#x200B;f(x)dx as a&#x2192;&#x221E;a \\to \\inftya&#x2192;&#x221E;. But you&#x2019;d be wrong! The actual definition is: &#x222B;&#x2212;&#x221E;&#x221E;f(x)&#x2009;dx=&#x222B;&#x2212;&#x221E;0f(x)&#x2009;dx+&#x222B;0&#x221E;f(x)&#x2009;dx,\\int_{-\\infty}^\\infty f(x) \\,dx= \\int_{-\\infty}^0 f(x) \\,dx+ \\int_0^\\infty f(x) \\,dx,&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;f(x)dx=&#x222B;&#x2212;&#x221E;0&#x200B;f(x)dx+&#x222B;0&#x221E;&#x200B;f(x)dx, provided that both improper integrals exist. To see why, suppose we wanted to calculate &#x222B;&#x2212;&#x221E;&#x221E;x&#x2009;dx\\int_{-\\infty}^\\infty x \\,dx&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;xdx. This integral does not exist, even though lim&#x2061;a&#x2192;&#x221E;&#x222B;&#x2212;aax&#x2009;dx=0\\lim_{a \\to \\infty} \\int_{-a}^a x \\,dx= 0lima&#x2192;&#x221E;&#x200B;&#x222B;&#x2212;aa&#x200B;xdx=0. The problem with saying that &#x222B;&#x2212;&#x221E;&#x221E;x&#x2009;dx\\int_{-\\infty}^\\infty x \\,dx&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;xdx equals 0 is that it depends entirely on how fast the upper and lower bounds are going to infinity. For example, lim&#x2061;a&#x2192;&#x221E;&#x222B;&#x2212;a2ax&#x2009;dx=&#x221E;\\lim_{a \\to \\infty} \\int_{-a}^{2a} x \\,dx= \\inftylima&#x2192;&#x221E;&#x200B;&#x222B;&#x2212;a2a&#x200B;xdx=&#x221E;. Without more information on exactly how fast the upper and lower bounds are going to infinity, &#x222B;&#x2212;&#x221E;&#x221E;x&#x2009;dx\\int_{-\\infty}^\\infty x \\,dx&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;xdx could equal anything. Unbounded functions: For technical reasons, we also run into problems when fff is unbounded. Suppose we&#x2019;re interested in integrating f(x)=1/xf(x) = 1/\\sqrt{x}f(x)=1/x&#x200B;, for example. At x=0x=0x=0, f(x)f(x)f(x) is undefined. Even if we were to define it, fff wouldn&#x2019;t be continuous or monotone no matter what we chose, which causes problems with integration. As you might guess, however, we can extend our definition of the integral to include 0 as a lower bound by taking the limit as the bound goes to zero (if the limit exists): &#x222B;01x&#x2212;1/2&#x2009;dx=lim&#x2061;a&#x2192;0&#x222B;a1x&#x2212;1/2&#x2009;dx=lim&#x2061;a&#x2192;02x&#x2223;a1=21&#x2212;lim&#x2061;a&#x2192;02a=2\\begin{align*} \\int_0^1 x^{-1/2} \\,dx&amp;= \\lim_{a \\to 0} \\int_a^1 x^{-1/2} \\,dx\\\\ &amp;= \\lim_{a \\to 0} 2\\sqrt{x} \\Big|_a^1 \\\\ &amp;= 2\\sqrt{1} - \\lim_{a \\to 0} 2\\sqrt{a} \\\\ &amp;= 2 \\end{align*}&#x222B;01&#x200B;x&#x2212;1/2dx&#x200B;=a&#x2192;0lim&#x200B;&#x222B;a1&#x200B;x&#x2212;1/2dx=a&#x2192;0lim&#x200B;2x&#x200B;&#x200B;a1&#x200B;=21&#x200B;&#x2212;a&#x2192;0lim&#x200B;2a&#x200B;=2&#x200B; Note that in this case, if we failed to realize that fff was not bounded over the integration region and blindly plugged in 0 anyway, it wouldn&#x2019;t make a difference &#x2013; we&#x2019;d get the same answer. However, this is not always true and if you ever run into a situation where this arises, it&#x2019;s important to know the proper definition. "],["integration-techniques.html", "2.8 Integration techniques", " 2.8 Integration techniques Every time we compute a derivative, we get a formula for integration. For example, f(x)=log&#x2061;(x2+2)&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;f&#x2032;(x)=2xx2+2.f(x) = \\log(x^2 + 2) \\implies f&apos;(x) = \\frac{2x}{x^2 + 2}.f(x)=log(x2+2)&#x27F9;f&#x2032;(x)=x2+22x&#x200B;. This is great news if we are ever faced with the problem of calculating &#x222B;ab2xx2+2&#x2009;dx,\\int_a^b \\frac{2x}{x^2 + 2} \\,dx,&#x222B;ab&#x200B;x2+22x&#x200B;dx, but if we need to calculate &#x222B;abf(x)&#x2009;dx\\int_a^b f(x) \\,dx&#x222B;ab&#x200B;f(x)dx, how can we reverse engineer a function F(x)F(x)F(x) so that its derivative is f(x)f(x)f(x)? Unfortunately, this task is sometimes easy, sometimes hard, and sometimes impossible (and you have no way of knowing in advance which situation you are in). One could create a huge table of integral formulas by taking derivatives of various things. Providing such a table is beyond the scope of this review, but such tables exist online and are useful resources to be aware of. Even with such a table, however, there are a few useful integration techniques to be familiar with. Among other things, (a) it may be faster to use one of these techniques than looking up an integral (b) you might not have access to such a table at the moment, and (c) the form that appears in the table might be slightly different than what you need, and you might have to use one of these techniques in combination with the table to compute the integral. Substitution By far the most important technique to be aware of is substitution. For example, we know that &#x222B;ex&#x2009;dx=ex\\int e^x \\,dx= e^x&#x222B;exdx=ex, but what if we have to find &#x222B;e2x&#x2009;dx\\int e^{2x} \\,dx&#x222B;e2xdx? Is it e2xe^{2x}e2x? The answer (and this is extremely important to understand, because it comes up in statistics all the time) is that no, it isn&#x2019;t. We can check this easily using the chain rule: the derivative of e2xe^{2x}e2x is 2e2x2e^{2x}2e2x, so &#x222B;e2x&#x2009;dx&#x2260;e2x\\int e^{2x} \\,dx\\ne e^{2x}&#x222B;e2xdx&#xE020;=e2x. In this case, it&#x2019;s also fairly clear what we need to do in order to fix the problem: &#x222B;e2x&#x2009;dx\\int e^{2x} \\,dx&#x222B;e2xdx must be 12e2x\\tfrac{1}{2}e^{2x}21&#x200B;e2x: there must be a 1/2 present to cancel the 2 that comes from the chain rule. Conceptually, letting u=2xu=2xu=2x, we can visualize what&#x2019;s going on here as follows. Each unit of uuu covers twice as much ground as a unit of xxx. If we don&#x2019;t do something to correct for this, we&#x2019;re going to artificially inflate the area under the curve integral (i.e., the integral). This is what&#x2019;s going on in the red region below, which clearly has greater area than the blue region (the integral we&#x2019;re trying to calculate). However, if we compensate for this &#x2013; we&#x2019;re stretching xxx out by a factor of 2, so we need to shrink the value of the function by a factor of 2 to preserve the correct area &#x2013; we get the green region, which has the same area as the original blue region. To formalize this thinking into a procedure, if u=g(x)u=g(x)u=g(x), then (this works for any differentiable function ggg) du=g&#x2032;(x)&#x2009;dxdu = g&apos;(x) \\,dxdu=g&#x2032;(x)dx Substitute uuu for g(x)g(x)g(x) and 1/g&#x2032;(x)&#x2009;du1/g&apos;(x) \\,du1/g&#x2032;(x)du for dxdxdx Take the integral Substitute g(x)g(x)g(x) back for uuu If we are calculating a definite integral, then instead of step 4, we can transform the limits of integration aaa and bbb to g(a)g(a)g(a) and g(b)g(b)g(b); this is usually preferable. As practice, use this procedure to calculate &#x222B;x(x2&#x2212;1)5&#x2009;dx.\\int x (x^2 - 1)^5 \\,dx.&#x222B;x(x2&#x2212;1)5dx. You should get 112(x2&#x2212;1)6\\tfrac{1}{12} (x^2 - 1)^6121&#x200B;(x2&#x2212;1)6. Integration by parts Just as the chain rule gave us substitution, the product rule gives us a formula called integration by parts, which is usually written in the form: &#x222B;u&#x2009;dv=uv&#x2212;&#x222B;v&#x2009;du.\\int u \\,dv = uv - \\int v \\,du.&#x222B;udv=uv&#x2212;&#x222B;vdu. As an example of integration by parts in action, suppose we want to integrate &#x222B;log&#x2061;x&#x2009;dx\\int \\log x \\,dx&#x222B;logxdx. We can write this as u=log&#x2061;xdv=dxdu=1x&#x2009;dxv=x\\begin{align*} u &amp;= \\log x &amp;\\qquad dv &amp;= dx \\\\ du &amp;= \\frac{1}{x} \\,dx&amp;\\qquad v &amp;= x \\end{align*}udu&#x200B;=logx=x1&#x200B;dx&#x200B;dvv&#x200B;=dx=x&#x200B; Thus, &#x222B;log&#x2061;x&#x2009;dx=xlog&#x2061;x&#x2212;&#x222B;xdxx=xlog&#x2061;x&#x2212;&#x222B;&#x2009;dx=xlog&#x2061;x&#x2212;x\\begin{align*} \\int \\log x \\,dx&amp;= x \\log x - \\int x \\frac{dx}{x} \\\\ &amp;= x \\log x - \\int \\,dx\\\\ &amp;= x \\log x - x \\end{align*}&#x222B;logxdx&#x200B;=xlogx&#x2212;&#x222B;xxdx&#x200B;=xlogx&#x2212;&#x222B;dx=xlogx&#x2212;x&#x200B; As practice, use this procedure to calculate &#x222B;xex&#x2009;dx.\\int x e^x \\,dx.&#x222B;xexdx. You should get xex&#x2212;exx e^x - e^xxex&#x2212;ex. Kernel trick The above techniques are useful, but in statistics it is often the case that you can avoid them entirely and calculate the answer much faster using something I will call the &#x201C;kernel trick&#x201D; (I am not aware of this idea having an official name). For example, suppose we need to calculate &#x222B;0&#x221E;e&#x2212;5x&#x2009;dx.\\int_0^\\infty e^{-5x} \\,dx.&#x222B;0&#x221E;&#x200B;e&#x2212;5xdx. Sure, we can use substitution, but most statisticians will find it easier to recognize that this is very similar to the exponential distribution, which (like all distributions) integrates to 1: &#x222B;0&#x221E;&#x3BB;e&#x2212;&#x3BB;x&#x2009;dx=1&#xA0;for&#xA0;all&#xA0;&#x3BB;&gt;0.\\int_0^\\infty \\lambda e^{-\\lambda x} \\,dx= 1 \\text{ for all } \\lambda&gt; 0.&#x222B;0&#x221E;&#x200B;&#x3BB;e&#x2212;&#x3BB;xdx=1&#xA0;for&#xA0;all&#xA0;&#x3BB;&gt;0. Applying this shortcut: &#x222B;0&#x221E;e&#x2212;5x&#x2009;dx=15&#x222B;0&#x221E;5e&#x2212;5x&#x2009;dx=15\\begin{align*} \\int_0^{\\infty} e^{-5x} \\,dx&amp;= \\frac{1}{5} \\int_0^\\infty 5 e^{-5x} \\,dx\\\\ &amp;= \\frac{1}{5} \\end{align*}&#x222B;0&#x221E;&#x200B;e&#x2212;5xdx&#x200B;=51&#x200B;&#x222B;0&#x221E;&#x200B;5e&#x2212;5xdx=51&#x200B;&#x200B; The kernel of a distribution is the part that has the variable we&#x2019;re integrating over. This is the only part that needs to match in order for the trick to work: we can always manipulate the constants as we did above. As another example, suppose we need to find &#x222B;&#x2212;&#x221E;&#x221E;e&#x2212;x2&#x2009;dx.\\int_{-\\infty}^\\infty e^{-x^2} \\,dx.&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;e&#x2212;x2dx. This is actually impossible to solve using any of the integration techniques above &#x2013; there is no elementary form form for its antiderivative. However, it has the kernel of a normal distribution: 1&#x3C3;2&#x3C0;exp&#x2061;{&#x2212;12(x&#x2212;&#x3BC;&#x3C3;)2}\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right\\}&#x3C3;2&#x3C0;&#x200B;1&#x200B;exp{&#x2212;21&#x200B;(&#x3C3;x&#x2212;&#x3BC;&#x200B;)2} Letting &#x3BC;=0\\mu=0&#x3BC;=0 and &#x3C3;=1/2\\sigma = 1/\\sqrt{2}&#x3C3;=1/2&#x200B;, we get &#x222B;&#x2212;&#x221E;&#x221E;e&#x2212;x2&#x2009;dx=&#x3C0;&#x222B;&#x2212;&#x221E;&#x221E;12&#x3C0;/2exp&#x2061;{&#x2212;12(x1/2)2}=&#x3C0;\\begin{align*} \\int_{-\\infty}^\\infty e^{-x^2} \\,dx&amp;= \\sqrt{\\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi} / \\sqrt{2}} \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{x}{1/\\sqrt{2}} \\right)^2 \\right\\} \\\\ &amp;= \\sqrt{\\pi} \\end{align*}&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;e&#x2212;x2dx&#x200B;=&#x3C0;&#x200B;&#x222B;&#x2212;&#x221E;&#x221E;&#x200B;2&#x3C0;&#x200B;/2&#x200B;1&#x200B;exp{&#x2212;21&#x200B;(1/2&#x200B;x&#x200B;)2}=&#x3C0;&#x200B;&#x200B; This may seem complicated at first, but I cannot emphasize enough how important it is to learn this. As a statistician you will become very familiar with these distributions and this will get easier and easier. Every fall, in a ritual as constant as the turning of the leaves, first-year graduate students labor away, trying to solve integrals using elaborate integration by parts techniques, and a professor or older graduate student will look at what they are doing and solve it in seconds using this trick. As practice, use this procedure to calculate &#x222B;0&#x221E;x2e&#x2212;x&#x2009;dx\\int_0^\\infty x^2 e^{-x} \\,dx&#x222B;0&#x221E;&#x200B;x2e&#x2212;xdx by using the kernel trick with respect to the gamma distribution, which has density function &#x3B2;&#x3B1;&#x393;(&#x3B1;)x&#x3B1;&#x2212;1e&#x2212;&#x3B2;x.\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}.&#x393;(&#x3B1;)&#x3B2;&#x3B1;&#x200B;x&#x3B1;&#x2212;1e&#x2212;&#x3B2;x. You should get &#x393;(3)\\Gamma(3)&#x393;(3), which is 2: &#x393;(&#x3B1;)=(&#x3B1;&#x2212;1)!\\Gamma(\\alpha) = (\\alpha-1)!&#x393;(&#x3B1;)=(&#x3B1;&#x2212;1)! if &#x3B1;\\alpha&#x3B1; is an integer. "],["sequences-and-series.html", "2.9 Sequences and series", " 2.9 Sequences and series A sequence is an ordered list of objects (usually numbers): x1,x2,&#x2026;,xnx_1, x_2, \\ldots, x_nx1&#x200B;,x2&#x200B;,&#x2026;,xn&#x200B;. A series is a special type of sequence that arises from the cumulative sum of another sequence: x1+x2+&#x22EF;+xnx_1 + x_2 + \\cdots + x_nx1&#x200B;+x2&#x200B;+&#x22EF;+xn&#x200B;, denoted &#x2211;i=1nxi\\sum_{i=1}^n x_i&#x2211;i=1n&#x200B;xi&#x200B;. The limiting behavior of sequences and series (what happens to them as n&#x2192;&#x221E;n \\to \\inftyn&#x2192;&#x221E;) is very important to statistics as we are often concerned with what happens to an estimate xnx_nxn&#x200B; as we collect more data (nnn here representing the number of observations). In particular, does xnx_nxn&#x200B; get closer and closer to the quantity we are trying to estimate? The study of infinite sequences and series is a big topic; this review is not exhaustive but covers the core definitions and commonly arising results that come up often in statistics. Finite series Most of this section centers on infinite sequences and series, but here are some important finite series to be familiar with (you should know the first three by heart, and know where to look up the rest of them when you need them): &#x2211;i=1n1=n&#x2211;i=1nc=nc&#x2211;i=1ni=n(n+1)2&#x2211;i=1ni2=n(n+1)(2n+1)6&#x2211;i=1ni3={n(n+1)2}2&#x2211;i=1ni4=n(n+1)(2n+1)(3n2+3n&#x2212;1)30\\begin{align*} \\sum_{i=1}^n 1 &amp;= n \\\\ \\sum_{i=1}^n c &amp;= nc \\\\ \\sum_{i=1}^n i &amp;= \\frac{n(n+1)}{2} \\\\ \\sum_{i=1}^n i^2 &amp;= \\frac{n(n+1)(2n+1)}{6} \\\\ \\sum_{i=1}^n i^3 &amp;= \\left\\{ \\frac{n(n+1)}{2} \\right\\}^2\\\\ \\sum_{i=1}^n i^4 &amp;= \\frac{n(n+1)(2n+1)(3n^2+3n-1)}{30}\\\\ \\end{align*}i=1&#x2211;n&#x200B;1i=1&#x2211;n&#x200B;ci=1&#x2211;n&#x200B;ii=1&#x2211;n&#x200B;i2i=1&#x2211;n&#x200B;i3i=1&#x2211;n&#x200B;i4&#x200B;=n=nc=2n(n+1)&#x200B;=6n(n+1)(2n+1)&#x200B;={2n(n+1)&#x200B;}2=30n(n+1)(2n+1)(3n2+3n&#x2212;1)&#x200B;&#x200B; Infinite sequences The limit of a sequence is very similar to the limit of a function that we have previously encountered. A sequence ana_nan&#x200B; has limit aaa if ana_nan&#x200B; gets closer and closer to aaa (i.e., the difference &#x2223;an&#x2212;a&#x2223;\\lvert a_n-a\\rvert&#x2223;an&#x200B;&#x2212;a&#x2223; gets smaller and smaller) as nnn goes to infinity. This is denoted an&#x2192;aa_n \\to aan&#x200B;&#x2192;a5. As with functions, an&#x2192;&#x221E;\\mathbf{a}_n \\to \\inftyan&#x200B;&#x2192;&#x221E; means that ana_nan&#x200B; just keeps getting bigger, with no bound. If the limit exists, we say that the sequence converges (or is convergent). Otherwise, we say the sequence diverges (or is divergent). If an&#x2192;&#x221E;\\mathbf{a}_n \\to \\inftyan&#x200B;&#x2192;&#x221E;, people often say that the &#x201C;limit is infinity&#x201D;, although keep in mind that if this happens, the sequence diverges (it just diverges in a particular way). All of the limit laws we discussed earlier for functions are equally valid when stated in terms of sequences. For example, if an&#x2192;aa_n \\to aan&#x200B;&#x2192;a and bn&#x2192;bb_n \\to bbn&#x200B;&#x2192;b, we have lim&#x2061;n&#x2192;&#x221E;{anbn}=ab,\\lim_{n \\to \\infty} \\{ a_n b_n \\} = ab,n&#x2192;&#x221E;lim&#x200B;{an&#x200B;bn&#x200B;}=ab, just like we did for functions. Techniques: It is often unclear what the limit of a ratio is &#x2013; both numerator and denominator could be going to infinity, or both going to zero. Two techniques to remember are dividing by the largest power and L&#x2019;H&#xF4;pital&#x2019;s rule. To illustrate the first: n2&#x2212;5n3+n+3=n&#x2212;1&#x2212;5n&#x2212;31+n&#x2212;2+3n&#x2212;3&#x2192;0\\begin{align*} \\frac{n^2 - 5}{n^3 + n + 3} &amp;= \\frac{n^{-1} - 5n^{-3}}{1 + n^{-2} + 3n^{-3}} \\\\ &amp;\\to 0 \\end{align*}n3+n+3n2&#x2212;5&#x200B;&#x200B;=1+n&#x2212;2+3n&#x2212;3n&#x2212;1&#x2212;5n&#x2212;3&#x200B;&#x2192;0&#x200B; To illustrate the second: lim&#x2061;n&#x2192;&#x221E;log&#x2061;nn=lim&#x2061;n&#x2192;&#x221E;n&#x2212;11=0\\begin{align*} \\lim_{n \\to \\infty} \\frac{\\log n}{n} &amp;= \\lim_{n \\to \\infty} \\frac{n^{-1}}{1} \\\\ &amp;= 0 \\end{align*}n&#x2192;&#x221E;lim&#x200B;nlogn&#x200B;&#x200B;=n&#x2192;&#x221E;lim&#x200B;1n&#x2212;1&#x200B;=0&#x200B; Recall that L&#x2019;H&#xF4;pital&#x2019;s rule only holds if the ratio of the derivatives converges and the original fraction is indeterminate. Some special sequences: It is useful to know the limits of some sequences, because they come up frequently: rn&#x2192;{0if&#xA0;&#x2223;r&#x2223;&lt;11if&#xA0;r=1diverges&#xA0;otherwiser^n \\to \\begin{cases} 0 \\quad \\text{if } \\lvert r\\rvert &lt; 1 \\\\ 1 \\quad \\text{if } r = 1 \\\\ \\text{diverges otherwise} \\end{cases}rn&#x2192;&#x23A9;&#x23A8;&#x23A7;&#x200B;0if&#xA0;&#x2223;r&#x2223;&lt;11if&#xA0;r=1diverges&#xA0;otherwise&#x200B; nr&#x2192;{0if&#xA0;r&lt;01if&#xA0;r=0diverges&#xA0;otherwisen^r \\to \\begin{cases} 0 \\quad \\text{if } r &lt; 0 \\\\ 1 \\quad \\text{if } r = 0 \\\\ \\text{diverges otherwise} \\end{cases}nr&#x2192;&#x23A9;&#x23A8;&#x23A7;&#x200B;0if&#xA0;r&lt;01if&#xA0;r=0diverges&#xA0;otherwise&#x200B; r1/n&#x2192;{1if&#xA0;r&gt;00if&#xA0;r=0undefined&#xA0;otherwiser^{1/n} \\to \\begin{cases} 1 \\quad \\text{if } r &gt; 0 \\\\ 0 \\quad \\text{if } r = 0 \\\\ \\text{undefined otherwise} \\end{cases}r1/n&#x2192;&#x23A9;&#x23A8;&#x23A7;&#x200B;1if&#xA0;r&gt;00if&#xA0;r=0undefined&#xA0;otherwise&#x200B; na(1+r)n&#x2192;0&#xA0;for&#xA0;all&#xA0;a&#xA0;if&#xA0;r&gt;0\\frac{n^a}{(1+r)^n} \\to 0 \\text{ for all } a \\text{ if } r &gt; 0(1+r)nna&#x200B;&#x2192;0&#xA0;for&#xA0;all&#xA0;a&#xA0;if&#xA0;r&gt;0 Monotone convergence theorem: Another way of establishing that a sequence converges is the monotone convergence theorem, which states that every bounded, monotonic sequence converges. Infinite series Similarly, we say that the series &#x2211;i=1&#x221E;ai\\sum_{i=1}^\\infty a_i&#x2211;i=1&#x221E;&#x200B;ai&#x200B; converges if the sequence of partial sums sn=&#x2211;i=1nais_n = \\sum_{i=1}^n a_isn&#x200B;=&#x2211;i=1n&#x200B;ai&#x200B; converges. If sn&#x2192;ss_n \\to ssn&#x200B;&#x2192;s, then sss is the called the sum of the series. Otherwise, the series is said to diverge. Note that if the series &#x2211;n=1&#x221E;an\\sum_{n=1}^\\infty a_n&#x2211;n=1&#x221E;&#x200B;an&#x200B; converges, then an&#x2192;0a_n \\to 0an&#x200B;&#x2192;0. The converse, however, is not true. For example, the following is known as the harmonic series: lim&#x2061;n&#x2192;&#x221E;&#x2211;i=1n1n=1+12+13+14&#x22EF;=&#x221E;.\\lim_{n \\to \\infty} \\sum_{i=1}^n \\frac{1}{n} = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} \\cdots = \\infty.n&#x2192;&#x221E;lim&#x200B;i=1&#x2211;n&#x200B;n1&#x200B;=1+21&#x200B;+31&#x200B;+41&#x200B;&#x22EF;=&#x221E;. Note that series obey some of the limit rules, but not all of them. For example, the following are true if &#x2211;n=1&#x221E;an=a\\sum_{n=1}^\\infty a_n = a&#x2211;n=1&#x221E;&#x200B;an&#x200B;=a and &#x2211;n=1&#x221E;bn=b\\sum_{n=1}^\\infty b_n = b&#x2211;n=1&#x221E;&#x200B;bn&#x200B;=b: &#x2211;n=1&#x221E;can=ca&#x2211;n=1&#x221E;(an+bn)=a+b&#x2211;n=1&#x221E;(an&#x2212;bn)=a&#x2212;b\\begin{align*} \\sum_{n=1}^\\infty c a_n &amp;= c a \\\\ \\sum_{n=1}^\\infty (a_n + b_n) &amp;= a + b \\\\ \\sum_{n=1}^\\infty (a_n - b_n) &amp;= a - b \\\\ \\end{align*}n=1&#x2211;&#x221E;&#x200B;can&#x200B;n=1&#x2211;&#x221E;&#x200B;(an&#x200B;+bn&#x200B;)n=1&#x2211;&#x221E;&#x200B;(an&#x200B;&#x2212;bn&#x200B;)&#x200B;=ca=a+b=a&#x2212;b&#x200B; However, not all of the limit laws carry over. In particular, if we see the quotient &#x2211;n=1&#x221E;anbn,\\sum_{n=1}^\\infty \\frac{a_n}{b_n},n=1&#x2211;&#x221E;&#x200B;bn&#x200B;an&#x200B;&#x200B;, we cannot conclude that its limit is a/ba/ba/b. This is a common mistake, so take some time to realize why this is false. There are two important series to be aware of: &#x2211;n=1&#x221E;rn&#x2212;1&#x2192;{11&#x2212;rif&#xA0;&#x2223;r&#x2223;&lt;1diverges&#xA0;otherwise(geometric&#xA0;series)\\sum_{n=1}^\\infty r^{n-1} \\to \\begin{cases} \\frac{1}{1-r} \\quad \\text{if } \\lvert r\\rvert &lt; 1 \\\\ \\text{diverges otherwise} \\end{cases} \\qquad \\text{(geometric series)}n=1&#x2211;&#x221E;&#x200B;rn&#x2212;1&#x2192;{1&#x2212;r1&#x200B;if&#xA0;&#x2223;r&#x2223;&lt;1diverges&#xA0;otherwise&#x200B;(geometric&#xA0;series) &#x2211;n=1&#x221E;nr&#x2192;{convergesif&#xA0;r&lt;&#x2212;1diverges&#xA0;otherwise\\sum_{n=1}^\\infty n^r \\to \\begin{cases} \\text{converges} \\quad \\text{if } r &lt; -1 \\\\ \\text{diverges otherwise} \\end{cases}n=1&#x2211;&#x221E;&#x200B;nr&#x2192;{convergesif&#xA0;r&lt;&#x2212;1diverges&#xA0;otherwise&#x200B; As before, we&#x2019;re skipping some technical details here because you shouldn&#x2019;t really need them for the first year. The topic is covered more rigorously here, which is intended for second-year students.&#x21A9;&#xFE0E; "],["partial-derivatives.html", "2.10 Partial derivatives", " 2.10 Partial derivatives As problems get more complicated, there are almost always multiple variables involved. Suppose we have the function f(x,y)=x2yf(x,y) = x^2yf(x,y)=x2y and we are told to find its derivative. This is an ambiguous request, and could mean several different things. For a function of several variables, a partial derivative of fff with respect to xxx means to calculate the ordinary derivative treating fff as a function of xxx alone with all the other variables held constant. This is denoted &#x2202;f/&#x2202;x\\partial f / \\partial x&#x2202;f/&#x2202;x. For the example above, taking the derivative with respect to xxx would yield 2xy2xy2xy, whereas taking the derivative with respect to yyy, would yield x2x^2x2. To give a further example, as well as introduce notation, for the function f(x,y,z)=x3z/yf(x,y,z) = x^3z/yf(x,y,z)=x3z/y, the partial derivatives with respect to x, y, and z would be written as: &#x2202;&#x2202;x(x3zy)=3x2zy&#x2202;&#x2202;y(x3zy)=&#x2212;x3zy2&#x2202;&#x2202;z(x3zy)=x3y\\begin{align*} \\frac{\\partial}{\\partial x} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{3x^2z}{y} \\\\ \\frac{\\partial}{\\partial y} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{-x^3z}{y^2} \\\\ \\frac{\\partial}{\\partial z} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{x^3}{y} \\end{align*}&#x2202;x&#x2202;&#x200B;(yx3z&#x200B;)&#x2202;y&#x2202;&#x200B;(yx3z&#x200B;)&#x2202;z&#x2202;&#x200B;(yx3z&#x200B;)&#x200B;=y3x2z&#x200B;=y2&#x2212;x3z&#x200B;=yx3&#x200B;&#x200B; In each partial derivative, two of the input variables are treated as constants. For example, when we take &#x2202;&#x2202;x\\frac{\\partial}{\\partial x}&#x2202;x&#x2202;&#x200B;, the z/yz/yz/y portion of the function is treated as a constant, and so on. Conceptually, a partial derivative represents the instantaneous rate of change in a function when only one of its input variables is changed while keeping all other input variables constant. Thus, fff is changing at the rate 3(2)2(4)/33(2)^2(4)/33(2)2(4)/3 as we increase xxx from, say, 2 to 2.001 when x=2x=2x=2, y=3y=3y=3, and z=4z=4z=4. Higher orders Partial derivatives can also be taken to the second order, such as &#x2202;2&#x2202;x2(x3zy)=&#x2202;&#x2202;x(&#x2202;&#x2202;x(x3zy))=&#x2202;&#x2202;x(3x2zy)=6xzy&#x2202;2&#x2202;z2(x3zy)=&#x2202;&#x2202;z(&#x2202;&#x2202;z(x3zy))=&#x2202;&#x2202;z(x3y)=0.\\begin{align*} \\frac{\\partial^2}{\\partial x^2} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{\\partial}{\\partial x}(\\frac{\\partial}{\\partial x}(\\frac{x^3z}{y})) \\\\ &amp;= \\frac{\\partial}{\\partial x}(\\frac{3x^2z}{y}) \\\\ &amp;= \\frac{6xz}{y} \\\\ \\\\ \\frac{\\partial^2}{\\partial z^2} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{\\partial}{\\partial z}(\\frac{\\partial}{\\partial z}(\\frac{x^3z}{y})) \\\\ &amp;= \\frac{\\partial}{\\partial z}(\\frac{x^3}{y}) \\\\ &amp;= 0. \\end{align*}&#x2202;x2&#x2202;2&#x200B;(yx3z&#x200B;)&#x2202;z2&#x2202;2&#x200B;(yx3z&#x200B;)&#x200B;=&#x2202;x&#x2202;&#x200B;(&#x2202;x&#x2202;&#x200B;(yx3z&#x200B;))=&#x2202;x&#x2202;&#x200B;(y3x2z&#x200B;)=y6xz&#x200B;=&#x2202;z&#x2202;&#x200B;(&#x2202;z&#x2202;&#x200B;(yx3z&#x200B;))=&#x2202;z&#x2202;&#x200B;(yx3&#x200B;)=0.&#x200B; Higher-order partial derivatives can also be &#x201C;mixed&#x201D;, meaning that we take a derivative first with respect to one variable, then with respect to a different variable. In terms of mathematical symbols, &#x2202;2f/&#x2202;x&#x2202;y\\partial^2 f / \\partial x \\partial y&#x2202;2f/&#x2202;x&#x2202;y is the same as &#x2202;/&#x2202;x(&#x2202;f/&#x2202;y)\\partial / \\partial x ( \\partial f / \\partial y)&#x2202;/&#x2202;x(&#x2202;f/&#x2202;y). A couple examples using the same f(x,y,z)=x3z/yf(x,y,z) = x^3z/yf(x,y,z)=x3z/y are given below: &#x2202;2&#x2202;x&#x2202;y(x3zy)=&#x2202;&#x2202;x&#x2202;&#x2202;y(x3zy)=&#x2202;&#x2202;x(&#x2212;x3zy2)=&#x2212;3x2zy2&#x2202;2&#x2202;z&#x2202;x(x3zy)=&#x2202;&#x2202;z&#x2202;&#x2202;x(x3zy)=&#x2202;&#x2202;z(3x2zy)=3x2y\\begin{align*} \\frac{\\partial^2}{\\partial x\\partial y} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{\\partial}{\\partial x}\\frac{\\partial}{\\partial y} (\\frac{x^3z}{y}) \\\\ &amp;= \\frac{\\partial}{\\partial x} (\\frac{-x^3z}{y^2}) \\\\ &amp;= \\frac{-3x^2z}{y^2} \\\\ \\\\ \\frac{\\partial^2}{\\partial z\\partial x} \\left( \\frac{x^3z}{y} \\right) &amp;= \\frac{\\partial}{\\partial z}\\frac{\\partial}{\\partial x} (\\frac{x^3z}{y}) \\\\ &amp;= \\frac{\\partial}{\\partial z} (\\frac{3x^2z}{y}) \\\\ &amp;= \\frac{3x^2}{y} \\end{align*}&#x2202;x&#x2202;y&#x2202;2&#x200B;(yx3z&#x200B;)&#x2202;z&#x2202;x&#x2202;2&#x200B;(yx3z&#x200B;)&#x200B;=&#x2202;x&#x2202;&#x200B;&#x2202;y&#x2202;&#x200B;(yx3z&#x200B;)=&#x2202;x&#x2202;&#x200B;(y2&#x2212;x3z&#x200B;)=y2&#x2212;3x2z&#x200B;=&#x2202;z&#x2202;&#x200B;&#x2202;x&#x2202;&#x200B;(yx3z&#x200B;)=&#x2202;z&#x2202;&#x200B;(y3x2z&#x200B;)=y3x2&#x200B;&#x200B; It is worth noting (and a bit surprising) that the order is irrelevant, a result known as Clairaut&#x2019;s Theorem. If &#x2202;2f/&#x2202;x&#x2202;y\\partial^2 f / \\partial x \\partial y&#x2202;2f/&#x2202;x&#x2202;y and &#x2202;2f/&#x2202;y&#x2202;x\\partial^2 f / \\partial y \\partial x&#x2202;2f/&#x2202;y&#x2202;x are both continuous, then &#x2202;2f&#x2202;x&#x2202;y=&#x2202;2f&#x2202;y&#x2202;x.\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}.&#x2202;x&#x2202;y&#x2202;2f&#x200B;=&#x2202;y&#x2202;x&#x2202;2f&#x200B;. Partial derivatives have many uses in statistics, but the main ideas above are all you really need to know for the first year. Later years will have an increasing focus on multivariable mathematical statistics. Gradients A closely related idea to the partial derivative is that of the gradient, which simply collects all of the partial derivatives of a function into a vector, symbolized by &#x2207;f(x)\\nabla f(\\mathbf{x})&#x2207;f(x), where x\\mathbf{x}x represents the vector (x1,x2,&#x2026;,xn)(x_1, x_2, \\ldots, x_n)(x1&#x200B;,x2&#x200B;,&#x2026;,xn&#x200B;): &#x2207;f(x1,...,xn)=[&#x2202;f(x)&#x2202;x1&#x2202;f(x)&#x2202;x2&#x22EE;&#x2202;f(x)&#x2202;xn]\\nabla f(x_1, ..., x_n) = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\end{bmatrix}&#x2207;f(x1&#x200B;,...,xn&#x200B;)=&#x200B;&#x2202;x1&#x200B;&#x2202;f(x)&#x200B;&#x2202;x2&#x200B;&#x2202;f(x)&#x200B;&#x22EE;&#x2202;xn&#x200B;&#x2202;f(x)&#x200B;&#x200B;&#x200B; Again, gradients will play a larger role in later years than they do in the first year, but familiarity with the basic idea now will help. One reason (among many) that gradients are useful is that in multiple dimensions, there are infinitely many ways we could change the input: we could increase xxx a little while decreasing yyy at the same rate but change zzz three times as fast, and so on. With the gradient, we can calculate these changes by taking the inner product of the gradient and direction: &#x2207;f(x)&#x22A4;d\\nabla f(\\mathbf{x}) ^{\\scriptstyle\\top}\\mathbf{d}&#x2207;f(x)&#x22A4;d, where d\\mathbf{d}d is the direction. "],["multiple-integrals.html", "2.11 Multiple integrals", " 2.11 Multiple integrals In statistics, we also (quite often!) need to integrate functions involving multiple variables. Considering trying to integrate the following function: f(x,y)=x2y.f(x,y) = x^2y.f(x,y)=x2y. Similar to partial derivatives, we could integrate with respect to xxx (treating yyy as a constant) or integrate with respect to yyy (treating xxx as constant). This should be relatively straightforward for you to get x3y/3x^3y/3x3y/3 and x2y2/2x^2y^2/2x2y2/2 in these two scenarios. However, we often need to integrate with respect to both xxx and yyy. Thankfully, this can be done one variable at a time: &#x222B;&#x222B;f(x,y)&#x2009;dx&#x2009;dy=&#x222B;{&#x222B;f(x,y)&#x2009;dx}&#x2009;dy=&#x222B;{&#x222B;f(x,y)&#x2009;dy}&#x2009;dx.\\begin{align*} \\int\\int f(x,y) \\,dx\\,dy&amp;= \\int \\left\\{ \\int f(x,y) \\,dx\\right\\} \\,dy\\\\ &amp;= \\int \\left\\{ \\int f(x,y) \\,dy\\right\\} \\,dx. \\end{align*}&#x222B;&#x222B;f(x,y)dxdy&#x200B;=&#x222B;{&#x222B;f(x,y)dx}dy=&#x222B;{&#x222B;f(x,y)dy}dx.&#x200B; As we saw with second-order partial derivatives, it turns out that if fff is continuous, the order of integration doesn&#x2019;t matter (this is known as Fubini&#x2019;s theorem); more on this later. Let&#x2019;s say we&#x2019;ve decided to integrate with respect to xxx first, then yyy (as we indicate when we write &#x2009;dx&#x2009;dy\\,dx\\,dydxdy): &#x222B;&#x222B;x2y&#x2009;dx&#x2009;dy=&#x222B;{x3y3}&#x2009;dy=x3y26\\begin{align*} \\int \\int x^2 y \\,dx\\,dy&amp;= \\int \\left\\{ \\frac{x^3y}{3} \\right\\} \\,dy\\\\ &amp;= \\frac{x^3y^2}{6} \\end{align*}&#x222B;&#x222B;x2ydxdy&#x200B;=&#x222B;{3x3y&#x200B;}dy=6x3y2&#x200B;&#x200B; Suppose now that we want to integrate f(x,y)=x2yf(x,y) = x^2yf(x,y)=x2y across the rectangular region 0&#x2264;x&#x2264;10 \\le x \\le 10&#x2264;x&#x2264;1 and 0&#x2264;y&#x2264;10 \\le y \\le 10&#x2264;y&#x2264;1 (i.e., to take the definite integral). This would go as follows: &#x222B;01&#x222B;01x2y&#x2009;dx&#x2009;dy=&#x222B;01{x3y3&#x2223;x=01}&#x2009;dy=&#x222B;01{(1)3y3&#x2212;(0)3y3}&#x2009;dy=&#x222B;01y3&#x2009;dy=126&#x2212;026=16\\begin{align*} \\int_0^1 \\int_0^1 x^2 y \\,dx\\,dy&amp;= \\int_0^1 \\left\\{ \\frac{x^3y}{3} \\Bigr\\rvert_{x=0}^1\\right\\} \\,dy\\\\ &amp;= \\int_0^1 \\left\\{ \\frac{(1)^3y}{3} - \\frac{(0)^3y}{3} \\right\\} \\,dy\\\\ &amp;= \\int_0^1 \\frac{y}{3} \\,dy\\\\ &amp;= \\frac{1^2}{6} - \\frac{0^2}{6} \\\\ &amp;= \\frac{1}{6} \\end{align*}&#x222B;01&#x200B;&#x222B;01&#x200B;x2ydxdy&#x200B;=&#x222B;01&#x200B;{3x3y&#x200B;&#x200B;x=01&#x200B;}dy=&#x222B;01&#x200B;{3(1)3y&#x200B;&#x2212;3(0)3y&#x200B;}dy=&#x222B;01&#x200B;3y&#x200B;dy=612&#x200B;&#x2212;602&#x200B;=61&#x200B;&#x200B; Conceptually, 16\\frac{1}{6}61&#x200B; represents the volume encompassed within the function x2yx^2yx2y across the rectangular region 0&#x2264;x&#x2264;10 \\leq x \\leq 10&#x2264;x&#x2264;1 and 0&#x2264;y&#x2264;10 \\leq y \\leq 10&#x2264;y&#x2264;1; this volume and the rectangular x,yx,yx,y region are depicted below: Multiple integrals can taken over any number of input variables: &#x222B;&#x22EF;&#x222B;f(x1,&#x2026;,xn)&#x2009;dx1&#x2026;&#x2009;dxn\\int \\cdots \\int f(x_1, \\ldots, x_n) \\,dx_1 \\ldots \\,dx_n&#x222B;&#x22EF;&#x222B;f(x1&#x200B;,&#x2026;,xn&#x200B;)dx1&#x200B;&#x2026;dxn&#x200B; As practice, integrate f(x,y,z)=x3z/yf(x,y,z) = x^3z/yf(x,y,z)=x3z/y across x, then y and then z (a triple integral). You should get x4z2log&#x2061;(y)/8x^4 z^2 \\log(y)/8x4z2log(y)/8. Non-rectangular regions A final complication to be aware of with multiple integrals is that the region over which we are integrating may not be rectangular. For example, let&#x2019;s consider our same function as before, f(x,y)=x2yf(x,y) = x^2 yf(x,y)=x2y, but now our region of integration will be 0&#x2264;x&#x2264;y&#x2264;10 \\leq x \\leq y \\leq 10&#x2264;x&#x2264;y&#x2264;1: Note that all the points in the green triangle (and only the points in the green triangle) satisfy 0&#x2264;x&#x2264;y&#x2264;10 \\leq x \\leq y \\leq 10&#x2264;x&#x2264;y&#x2264;1. At x=0.4x = 0.4x=0.4 for instance, only points where yyy is greater than or equal to 0.4 and less than 1 are shaded green. In comparison with the earlier rectangular region, note that the lower right half is no longer included. The procedure to integrate &#x222B;&#x222B;f(x,y)&#x2009;dx&#x2009;dy\\int \\int f(x, y) \\,dx\\,dy&#x222B;&#x222B;f(x,y)dxdy is the same as before, but we need to pay closer attention to the bounds. Since we&#x2019;re starting with respect to xxx, let&#x2019;s figure out the appropriate xxx bounds. Visualizing the green region plotted above is immensely helpful in determining the upper and lower bounds for xxx (you don&#x2019;t necessarily need to use a computer, but you should always draw the region of integration if it is not rectangular). To determine the lower and upper bounds of the inner integral with respect to xxx, we need to answer the question: at any given value of yyy, what values of xxx are within the green region? The answer is: 0&#x2264;x&#x2264;y0 \\le x \\le y0&#x2264;x&#x2264;y. For instance, at y=0.2y = 0.2y=0.2, xxx can range from 0 to 0.2. Regarding the range of yyy (the outer integral), we need to include the entire range of 0 to 1 in order to capture the entire triangular region. To visualize this, imagine drawing a line over the valid xxx values, and repeating this process for every yyy value in the range of 0 to 1. A depiction of what this would look like is provided below (hopefully it is clear that the collection of lines would yield the triangle): Evaluating the integral, we therefore have: &#x222B;01&#x222B;0yx2y&#x2009;dx&#x2009;dy=&#x222B;01{x3y3&#x2223;x=0y}&#x2009;dy=&#x222B;01{(y)3y3&#x2212;(0)3y3}&#x2009;dy=&#x222B;01y43&#x2009;dy=1515&#x2212;0515=115\\begin{align*} \\int_0^1 \\int_0^y x^2 y \\,dx\\,dy&amp;= \\int_0^1 \\left\\{ \\frac{x^3y}{3} \\Bigr\\rvert_{x=0}^y \\right\\} \\,dy\\\\ &amp;= \\int_0^1 \\left\\{ \\frac{(y)^3y}{3} - \\frac{(0)^3y}{3} \\right\\} \\,dy\\\\ &amp;= \\int_0^1 \\frac{y^4}{3} \\,dy\\\\ &amp;= \\frac{1^5}{15} - \\frac{0^5}{15} \\\\ &amp;= \\frac{1}{15} \\end{align*}&#x222B;01&#x200B;&#x222B;0y&#x200B;x2ydxdy&#x200B;=&#x222B;01&#x200B;{3x3y&#x200B;&#x200B;x=0y&#x200B;}dy=&#x222B;01&#x200B;{3(y)3y&#x200B;&#x2212;3(0)3y&#x200B;}dy=&#x222B;01&#x200B;3y4&#x200B;dy=1515&#x200B;&#x2212;1505&#x200B;=151&#x200B;&#x200B; Intuitively, this makes sense &#x2013; our region is smaller, so the volume should be smaller (1/15 instead of 1/6). As we mentioned earlier, the order of integration doesn&#x2019;t matter. As practice, integrate x2yx^2 yx2y over the region 0&#x2264;x&#x2264;y&#x2264;10 \\leq x \\leq y \\leq 10&#x2264;x&#x2264;y&#x2264;1, but this time start by integrating over yyy. Obviously, you should still get 1/15. Hint: to help visualize things, you should draw a figure that looks like this: If you want even more practice, trying integrating x2yx^2 yx2y over the region 0&#x2264;y&#x2264;x&#x2264;10 \\le y \\le x \\leq 10&#x2264;y&#x2264;x&#x2264;1. You should get 1/10 (i.e., 1/6&#x2212;1/151/6 - 1/151/6&#x2212;1/15). Note that while either order gives the same answer, they do not necessarily involve the same amount of work. It is often the case that one route is much easier to calculate than the other, so keep this in mind if one of the integrals becomes difficult to calculate. And remember to draw those regions of integration (really can&#x2019;t stress that enough)! "],["matrix-algebra.html", "3 Matrix algebra", " 3 Matrix algebra In this chapter, we will review a number of definitions and results that you should be familiar with from a course on linear algebra. This review will not focus at all on the theory of linear algebra, only on the practical matter of manipulating matrices &#x2013; hence the title &#x201C;matrix algebra&#x201D;, which is intended to reflect a focus on algebraic manipulation, not the abstract characterization of linear functions and transformations. Matrices come up constantly in statistics. Before we begin the review, I&#x2019;ll point out two reasons for this. Representing data The data we collect in studies typically has the form of a matrix: for every subject, we collect information on a number of variables (age, weight, blood pressure, etc.). By convention, we write this in matrix form where every subject is assigned a row of the matrix and each variable is assigned a column. For a study with nnn subjects and ppp variables, we can represent the data we have collected in the form of a n&#xD7;pn \\times pn&#xD7;p matrix. Multivariable manipulations As we saw in the previous chapter, calculus is important for finding optimal solutions. When multiple variables are present, however, we need to simultaneously solve over all the variables to find the maximum or minimum. For example, if there are two unknown parameters &#x3B8;1\\theta_1&#x3B8;1&#x200B; and &#x3B8;2\\theta_2&#x3B8;2&#x200B;, and we want to find the most likely values of &#x3B8;1\\theta_1&#x3B8;1&#x200B; and &#x3B8;2\\theta_2&#x3B8;2&#x200B; based on data we&#x2019;ve collected, we can&#x2019;t just find the most likely value of &#x3B8;1\\theta_1&#x3B8;1&#x200B; and then find the most likely value of &#x3B8;2\\theta_2&#x3B8;2&#x200B; &#x2013; the most likely value of &#x3B8;1\\theta_1&#x3B8;1&#x200B; is going to depend on what &#x3B8;2\\theta_2&#x3B8;2&#x200B; is. If ten parameters are involved, this process is going to seem hopeless unless we can manipulate and solve all ten equations simultaneously. Thankfully, manipulating and solving multiple equations at the same time is what matrix algebra is all about. "],["definitions-and-conventions.html", "3.1 Definitions and conventions", " 3.1 Definitions and conventions A matrix is a collection of numbers arranged in a rectangular array of rows and columns, such as [324&#x2212;1&#x2212;12]\\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 4 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right]&#x200B;34&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B; A matrix with rrr rows and ccc columns is said to be an r&#xD7;cr \\times cr&#xD7;c matrix (e.g., the matrix above is a 3&#xD7;23 \\times 23&#xD7;2 matrix). In the case where a matrix has just a single row or column, it is said to be a vector, such as [3&#x2212;1]\\left[ \\begin{array}{r} 3 \\\\ -1 \\end{array} \\right][3&#x2212;1&#x200B;] Conventionally, vectors and matrices are denoted in lower- and upper-case boldface, respectively (e.g., xxx is a scalar, x\\mathbf{x}x is a vector, and X\\mathbf{X}X is a matrix). In addition, vectors are taken to be column vectors &#x2013; i.e., a vector of nnn numbers is an n&#xD7;1n \\times 1n&#xD7;1 matrix, not a 1&#xD7;n1 \\times n1&#xD7;n matrix. The ijijijth element of a matrix M\\mathbf{M}M is denoted by MijM_{ij}Mij&#x200B; or (M)ij(\\mathbf{M})_{ij}(M)ij&#x200B;. For example, letting M\\mathbf{M}M denote the above matrix, M11=3M_{11}=3M11&#x200B;=3, (M)32=2(\\mathbf{M})_{32}=2(M)32&#x200B;=2, and so on. Similarly, the jjjth element of a vector v\\mathbf{v}v is denoted vjv_jvj&#x200B;; e.g., letting v\\mathbf{v}v denote the above vector, v1=3v_1 = 3v1&#x200B;=3. "],["basic-operations.html", "3.2 Basic operations", " 3.2 Basic operations Transposition It is often useful to switch the rows and columns of a matrix around. The resulting matrix is called the transpose of the original matrix, and denoted with a superscript &#x22A4;^{\\scriptstyle\\top}&#x22A4; or an apostrophe &#x2032;&apos;&#x2032;: M=[324&#x2212;1&#x2212;12]M&#x22A4;=[34&#x2212;12&#x2212;12]\\mathbf{M}= \\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 4 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right] \\qquad \\mathbf{M}^{\\scriptstyle\\top}=\\left[ \\begin{array}{rrr} 3 &amp; 4 &amp; -1 \\\\ 2 &amp; -1 &amp; 2 \\end{array} \\right]M=&#x200B;34&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B;M&#x22A4;=[32&#x200B;4&#x2212;1&#x200B;&#x2212;12&#x200B;] Note that Mij=Mji&#x22A4;M_{ij}=M ^{\\scriptstyle\\top}_{ji}Mij&#x200B;=Mji&#x22A4;&#x200B;, and that if M\\mathbf{M}M is an r&#xD7;cr \\times cr&#xD7;c matrix, M&#x22A4;\\mathbf{M}^{\\scriptstyle\\top}M&#x22A4; is a c&#xD7;rc \\times rc&#xD7;r matrix. Addition There are two kinds of addition operations for matrices. The first is scalar addition: M+2=[3+22+24+2&#x2212;1+2&#x2212;1+22+2]=[546114]\\mathbf{M}+ 2 = \\left[ \\begin{array}{rr} 3+2 &amp; 2+2 \\\\ 4+2 &amp; -1+2 \\\\ -1+2 &amp; 2+2 \\end{array} \\right] = \\left[ \\begin{array}{rr } 5 &amp; 4 \\\\ 6 &amp; 1 \\\\ 1 &amp; 4 \\end{array} \\right]M+2=&#x200B;3+24+2&#x2212;1+2&#x200B;2+2&#x2212;1+22+2&#x200B;&#x200B;=&#x200B;561&#x200B;414&#x200B;&#x200B; The other kind is matrix addition: M+M=[324&#x2212;1&#x2212;12]+[324&#x2212;1&#x2212;12]=[648&#x2212;2&#x2212;24]\\mathbf{M}+ \\mathbf{M}= \\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 4 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right] + \\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 4 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right]= \\left[ \\begin{array}{rr} 6 &amp; 4 \\\\ 8 &amp; -2 \\\\ -2 &amp; 4 \\end{array} \\right]M+M=&#x200B;34&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B;+&#x200B;34&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B;=&#x200B;68&#x2212;2&#x200B;4&#x2212;24&#x200B;&#x200B; Formally, (A+B)ij=Aij+Bij(\\mathbf{A}+\\mathbf{B})_{ij} = A_{ij} + B_{ij}(A+B)ij&#x200B;=Aij&#x200B;+Bij&#x200B;. Note that only matrices of the same dimension can be added to each other &#x2013; there is no such thing as adding a 4&#xD7;54 \\times 54&#xD7;5 matrix to a 2&#xD7;92 \\times 92&#xD7;9 matrix. Multiplication There are also two common kinds of multiplication for matrices. The first is scalar multiplication: 4M=4[324&#x2212;1&#x2212;12]=[12816&#x2212;4&#x2212;48]4\\mathbf{M}= 4\\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 4 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right] = \\left[ \\begin{array}{rr } 12 &amp; 8 \\\\ 16 &amp; -4 \\\\ -4 &amp; 8 \\end{array} \\right]4M=4&#x200B;34&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B;=&#x200B;1216&#x2212;4&#x200B;8&#x2212;48&#x200B;&#x200B; Formally, (cM)ij=cMij(c\\mathbf{M})_{ij} = cM_{ij}(cM)ij&#x200B;=cMij&#x200B;. The other kind is matrix multiplication. The product of two matrices, AB\\mathbf{A}\\mathbf{B}AB, is defined by multiplying all of A\\mathbf{A}A&#x2019;s rows by B\\mathbf{B}B&#x2019;s columns in the following manner: (AB)ik=&#x2211;jAijBjk(\\mathbf{A}\\mathbf{B})_{ik} = \\sum_j A_{ij}B_{jk}(AB)ik&#x200B;=j&#x2211;&#x200B;Aij&#x200B;Bjk&#x200B; [1214&#x2212;10][320&#x2212;1&#x2212;12]=[22129]\\left[ \\begin{array}{rrr} 1 &amp; 2 &amp; 1 \\\\ 4 &amp; -1 &amp; 0 \\end{array} \\right] \\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 0 &amp; -1 \\\\ -1 &amp; 2 \\end{array} \\right]= \\left[ \\begin{array}{rr} 2 &amp; 2 \\\\ 12 &amp; 9 \\end{array} \\right][14&#x200B;2&#x2212;1&#x200B;10&#x200B;]&#x200B;30&#x2212;1&#x200B;2&#x2212;12&#x200B;&#x200B;=[212&#x200B;29&#x200B;] Note that matrix multiplication is only defined if the number of columns of A\\mathbf{A}A matches the number of rows of B\\mathbf{B}B, and that if A\\mathbf{A}A is an m&#xD7;nm \\times nm&#xD7;n matrix and B\\mathbf{B}B is an n&#xD7;pn \\times pn&#xD7;p matrix, then AB\\mathbf{A}\\mathbf{B}AB is an m&#xD7;pm \\times pm&#xD7;p matrix. The following elementary algebra rules carry over to matrix algebra: A+B=B+A(A+B)+C=A+(B+C)(AB)C=A(BC)A(B+C)=AB+ACk(A+B)=kA+kB\\begin{align*} \\mathbf{A}+\\mathbf{B}&amp;= \\mathbf{B}+\\mathbf{A}&amp; (\\mathbf{A}+\\mathbf{B})+\\mathbf{C}&amp;=\\mathbf{A}+(\\mathbf{B}+\\mathbf{C}) \\\\ (\\mathbf{A}\\mathbf{B})\\mathbf{C}&amp;= \\mathbf{A}(\\mathbf{B}\\mathbf{C}) &amp; \\mathbf{A}(\\mathbf{B}+\\mathbf{C})&amp;=\\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}\\\\ k(\\mathbf{A}+\\mathbf{B}) &amp;= k\\mathbf{A}+k\\mathbf{B} \\end{align*}A+B(AB)Ck(A+B)&#x200B;=B+A=A(BC)=kA+kB&#x200B;(A+B)+CA(B+C)&#x200B;=A+(B+C)=AB+AC&#x200B; One important exception, however, is that AB&#x2260;BA\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}AB&#xE020;=BA; the order of matrix multiplication matters, and we must remember to, for instance, &#x201C;left multiply&#x201D; both sides of an equation by a matrix M\\mathbf{M}M to preserve equality. Inner and outer products Suppose u\\mathbf{u}u and v\\mathbf{v}v are two n&#xD7;1n \\times 1n&#xD7;1 vectors. We can&#x2019;t multiply them in the sense defined above, uv\\mathbf{u}\\mathbf{v}uv, because the number of columns of u\\mathbf{u}u, 1, doesn&#x2019;t match the number of rows of v\\mathbf{v}v, n.&#xA0;However, there are two ways in which vectors of the same dimension can be multiplied. The first is called the inner product (also, the &#x201C;cross product&#x201D;): u&#x22A4;v=&#x2211;jujvj[32][2&#x2212;1]=6&#x2212;2=4.\\begin{align*} \\mathbf{u}^{\\scriptstyle\\top}\\mathbf{v}&amp;= \\sum_j u_j v_j \\\\ \\left[\\begin{array}{rr} 3 &amp; 2 \\end{array} \\right] \\left[\\begin{array}{r} 2 \\\\ -1 \\end{array}\\right] &amp;= 6 - 2 = 4. \\end{align*}u&#x22A4;v[3&#x200B;2&#x200B;][2&#x2212;1&#x200B;]&#x200B;=j&#x2211;&#x200B;uj&#x200B;vj&#x200B;=6&#x2212;2=4.&#x200B; Note that when we multiply matrices, the element (AB)ij(\\mathbf{A}\\mathbf{B})_{ij}(AB)ij&#x200B; is equal to the inner product of the ith row of A\\mathbf{A}A and the jth column of BBB. The second way of multiplying two vectors is called the outer product: (uv&#x22A4;)ij=uivj[32][2&#x2212;1]=[6&#x2212;34&#x2212;2]\\begin{align*} (\\mathbf{u}\\mathbf{v}^{\\scriptstyle\\top})_{ij} &amp;= u_i v_j \\\\ \\left[\\begin{array}{r} 3 \\\\ 2 \\end{array} \\right] \\left[\\begin{array}{rr} 2 &amp; -1 \\end{array}\\right] &amp;= \\left[\\begin{array}{rr} 6 &amp; -3 \\\\ 4 &amp; -2 \\end{array}\\right] \\end{align*}(uv&#x22A4;)ij&#x200B;[32&#x200B;][2&#x200B;&#x2212;1&#x200B;]&#x200B;=ui&#x200B;vj&#x200B;=[64&#x200B;&#x2212;3&#x2212;2&#x200B;]&#x200B; Note that the inner product returns a scalar number, while the outer product returns an n&#xD7;nn \\times nn&#xD7;n matrix. "],["special-matrices.html", "3.3 Special matrices", " 3.3 Special matrices In the special case where a matrix has the same numbers of rows and columns, it is said to be square. If A&#x22A4;=A\\mathbf{A}^{\\scriptstyle\\top}=\\mathbf{A}A&#x22A4;=A, the matrix is said to be symmetric. Symmetric:[122&#x2212;1]Not&#xA0;symmetric:[320&#x2212;1]\\text{Symmetric:} \\left[ \\begin{array}{rr} 1 &amp; 2 \\\\ 2 &amp; -1 \\end{array} \\right] \\qquad \\text{Not symmetric:} \\left[ \\begin{array}{rr} 3 &amp; 2 \\\\ 0 &amp; -1 \\end{array} \\right]Symmetric:[12&#x200B;2&#x2212;1&#x200B;]Not&#xA0;symmetric:[30&#x200B;2&#x2212;1&#x200B;] Note that a matrix cannot be symmetric unless it is square. The elements AiiA_{ii}Aii&#x200B; of a matrix are called its diagonal entries; a matrix for which Aij=0A_{ij} = 0Aij&#x200B;=0 for all i&#x2260;ji \\neq ji&#xE020;=j is said to be a diagonal matrix: [1000&#x2212;10005]\\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\end{array} \\right]&#x200B;100&#x200B;0&#x2212;10&#x200B;005&#x200B;&#x200B; Consider in particular the following diagonal matrix: I=[100010001]\\mathbf{I}= \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]I=&#x200B;100&#x200B;010&#x200B;001&#x200B;&#x200B; Note that this matrix has the interesting property that (AI)ij=Aij(\\mathbf{A}\\mathbf{I})_{ij}=A_{ij}(AI)ij&#x200B;=Aij&#x200B; for all i,&#x2009;ji, \\, ji,j; in other words, AI=IA=A\\mathbf{A}\\mathbf{I}=\\mathbf{I}\\mathbf{A}=\\mathbf{A}AI=IA=A. Because of this property, I\\mathbf{I}I is referred to as the identity matrix. Some other notations which are commonly used are 1\\mathbf{1}1, the vector (or matrix) of 1s, and 0\\mathbf{0}0, the vector (or matrix) of zeros: 1=[1&#xA0;1&#xA0;1]0=[0&#xA0;0&#xA0;0]\\mathbf{1}= \\left[ \\begin{array}{rrr} 1\\ 1\\ 1 \\end{array} \\right] \\qquad \\mathbf{0}= \\left[ \\begin{array}{rrr} 0\\ 0\\ 0 \\end{array} \\right]1=[1&#xA0;1&#xA0;1&#x200B;]0=[0&#xA0;0&#xA0;0&#x200B;] The dimensions of these matrices is sometimes explicitly specified, as in 02&#xD7;2\\mathbf{0}_{2 \\times 2}02&#xD7;2&#x200B;, I5&#xD7;5\\mathbf{I}_{5 \\times 5}I5&#xD7;5&#x200B;, or 14&#xD7;1\\mathbf{1}_{4 \\times 1}14&#xD7;1&#x200B;. Other times it is obvious from context what the dimensions must be. Finally, the vector ej\\mathbf{e}_jej&#x200B; is also useful: it has element ej=1e_j=1ej&#x200B;=1 and ek=0e_k=0ek&#x200B;=0 for all other elements: e2=[010].\\mathbf{e}_2 = \\left[ \\begin{array}{rrr} 0 \\\\ 1 \\\\ 0 \\end{array} \\right].e2&#x200B;=&#x200B;010&#x200B;&#x200B;. This is useful for selecting a single element of a vector: u&#x22A4;e3=u3\\mathbf{u}^{\\scriptstyle\\top}\\mathbf{e}_3 = u_3u&#x22A4;e3&#x200B;=u3&#x200B;. "],["inversion-and-related-concepts.html", "3.4 Inversion and related concepts", " 3.4 Inversion and related concepts Suppose Ax=B\\mathbf{A}\\mathbf{x}=\\mathbf{B}Ax=B and we want to solve for x\\mathbf{x}x &#x2026; can we &#x201C;divide&#x201D; by A\\mathbf{A}A? The answer is: &#x201C;sort of&#x201D;. There is no such thing as matrix division, but we can multiply both sides by the inverse of A\\mathbf{A}A. If a matrix A&#x2212;1\\mathbf{A}^{-1}A&#x2212;1 satisfies AA&#x2212;1=A&#x2212;1A=I\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}AA&#x2212;1=A&#x2212;1A=I, then A&#x2212;1\\mathbf{A}^{-1}A&#x2212;1 is the inverse of A\\mathbf{A}A. If we know what A&#x2212;1\\mathbf{A}^{-1}A&#x2212;1 is, then x=A&#x2212;1B\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{B}x=A&#x2212;1B. Note that x\\mathbf{x}x is not equal to BA&#x2212;1\\mathbf{B}\\mathbf{A}^{-1}BA&#x2212;1; we need to left multiply by the inverse and the order of multiplication matters. If two vectors u\\mathbf{u}u and v\\mathbf{v}v satisfy u&#x22A4;v=0\\mathbf{u}^{\\scriptstyle\\top}\\mathbf{v}=0u&#x22A4;v=0, they are said to be orthogonal to each other. If all the columns and rows of a matrix A\\mathbf{A}A are orthogonal to each other and satisfy a&#x22A4;a=1\\mathbf{a}^{\\scriptstyle\\top}\\mathbf{a}= 1a&#x22A4;a=1, then A\\mathbf{A}A (transposed) can serve as its own inverse: A&#x22A4;A=AA&#x22A4;=I\\mathbf{A}^{\\scriptstyle\\top}\\mathbf{A}=\\mathbf{A}\\mathbf{A}^{\\scriptstyle\\top}=\\mathbf{I}A&#x22A4;A=AA&#x22A4;=I. In this case, the matrix A\\mathbf{A}A is said to be an orthogonal matrix. If a matrix X\\mathbf{X}X is not square, then it is possible that X&#x22A4;X=I\\mathbf{X}^{\\scriptstyle\\top}\\mathbf{X}=\\mathbf{I}X&#x22A4;X=I but XX&#x22A4;&#x2260;I\\mathbf{X}\\mathbf{X}^{\\scriptstyle\\top}\\neq \\mathbf{I}XX&#x22A4;&#xE020;=I; in this case, the matrix is said to be column orthogonal, although in statistics it is common to refer to these matrices as orthogonal also. A somewhat related definition is that a matrix is said to be idempotent if AA=A\\mathbf{A}\\mathbf{A}=\\mathbf{A}AA=A. Does every matrix have one and only one inverse? If a matrix has an inverse, it is said to be invertible &#x2013; all invertible matrices have exactly one, unique inverse. However, not every matrix is invertible. For example, there are no values of a,b,ca, b, ca,b,c, and ddd that satisfy [2412][abcd]=[1001]\\left[ \\begin{array}{rr} 2 &amp; 4 \\\\ 1 &amp; 2 \\end{array} \\right] \\left[ \\begin{array}{rr} a &amp; b \\\\ c &amp; d \\end{array} \\right]= \\left[ \\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right][21&#x200B;42&#x200B;][ac&#x200B;bd&#x200B;]=[10&#x200B;01&#x200B;] Why doesn&#x2019;t this matrix have an inverse? There are four equations and four unknowns, but some of those equations contradict each other. The term for this situation is linear dependence. If you have a collection of vectors v1,v2,&#x2026;,vn\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_nv1&#x200B;,v2&#x200B;,&#x2026;,vn&#x200B;, then you can form new vectors from linear combinations of the old vectors: c1v1+c2v2+&#x22EF;+cnvnc_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_n\\mathbf{v}_nc1&#x200B;v1&#x200B;+c2&#x200B;v2&#x200B;+&#x22EF;+cn&#x200B;vn&#x200B;. A collection of vectors is said to be linearly independent if none of them can be written as a linear combination of the others; if it can, then they are linearly dependent. This is the key to whether a matrix is invertible or not: a matrix A\\mathbf{A}A is invertible if and only if its columns (or rows) are linearly independent. Note that the columns of our earlier matrix were not linearly independent, since 2(21)=(42)2(2 \\quad 1)=(4 \\quad 2)2(21)=(42). The rank of a matrix is the number of linearly independent columns (or rows) it has; if they&#x2019;re all linearly independent, then the matrix is said to be of full rank. Additional helpful identities: (A+B)&#x22A4;=A&#x22A4;+B&#x22A4;(AB)&#x22A4;=B&#x22A4;A&#x22A4;(AB)&#x2212;1=B&#x2212;1A&#x2212;1(A&#x22A4;)&#x2212;1=(A&#x2212;1)&#x22A4;\\begin{align*} (\\mathbf{A}+\\mathbf{B}) ^{\\scriptstyle\\top}&amp;= \\mathbf{A}^{\\scriptstyle\\top}+ \\mathbf{B}^{\\scriptstyle\\top}\\\\ (\\mathbf{A}\\mathbf{B}) ^{\\scriptstyle\\top}&amp;= \\mathbf{B}^{\\scriptstyle\\top}\\mathbf{A}^{\\scriptstyle\\top}\\\\ (\\mathbf{A}\\mathbf{B})^{-1} &amp;= \\mathbf{B}^{-1}\\mathbf{A}^{-1} \\\\ (\\mathbf{A}^{\\scriptstyle\\top})^{-1} &amp;= (\\mathbf{A}^{-1}) ^{\\scriptstyle\\top} \\end{align*}(A+B)&#x22A4;(AB)&#x22A4;(AB)&#x2212;1(A&#x22A4;)&#x2212;1&#x200B;=A&#x22A4;+B&#x22A4;=B&#x22A4;A&#x22A4;=B&#x2212;1A&#x2212;1=(A&#x2212;1)&#x22A4;&#x200B; "],["analysis.html", "4 Analysis", " 4 Analysis The material in chapters 2 and 3 is intended as review for incoming graduate students to prepare them for courses they will take their first year in the program. For students in our PhD program, there is an additional sequence of courses (BIOS 7110 and BIOS 7250) that covers the mathematical foundations of statistics in greater depth. For this material, an understanding of analysis is important. Analysis is concerned with the same topics as calculus, but calculus focuses on tools from a user perspective (&#x201C;how do I calculate a derivative?&#x201D;) whereas analysis focuses on theoretical properties (e.g., proving theorems about derivatives and differentiability). So, the table of contents here might appear similar to chapter 2, but the focus is quite different. Furthermore, constructing abstract proofs involves a rather different set of skills than deriving results in calculus or linear algebra, so it&#x2019;s also important to cover techniques and terminology that arise in constructing proofs. This is especially important to read if you have never taken a course in which you were asked to construct mathematical proofs. If you&#x2019;ve never had a course in real analysis, or it&#x2019;s been a while and you&#x2019;ve forgotten, this document should be useful if you&#x2019;re thinking about taking Likelihood Theory. REMINDER: If you are an incoming first-year student, you don&#x2019;t need to worry about this material yet! Just focus on chapters 2 and 3. Before we begin, a note on style. Most textbooks and papers provide proofs in an unstructured paragraph style. For the purposes of learning how to prove things, however, I recommend a more structured approach based on making consecutive explicit statements with explicit justifications. There are several reasons for this: When you finish a structured proof, it is very clear exactly which conditions were required, why they were required, and what supporting theorems or results were used. When you&#x2019;re learning in a course, this is extremely valuable as it will be more clear to you how everything is connected. The other major reason is that it&#x2019;s much harder to make a mistake in a structured proof. This doesn&#x2019;t make the proof easier, it just means that in an unstructured proof, one can easily skip steps without realizing it. We&#x2019;ll see some examples of this later. It&#x2019;s also very beneficial from a grading and feedback perspective, as it makes it much clearer to the person grading the proof whether you understand all the pieces or not. "],["there-exists-and-for-all.html", "4.1 There exists and For all", " 4.1 There exists and For all Many proofs in theoretical statistics involve convergence. A loose definition was provided earlier: as we collect more and more data, we can be increasingly sure that certain things will happen. We&#x2019;ll discuss probabilistic convergence in the course, but before that, it will help to be familiar with the rigorous definition of (deterministic) convergence. Here&#x2019;s the definition; we&#x2019;ll come back to it later. Definition. A sequence x1,x2,&#x2026;x_1, x_2, \\ldotsx1&#x200B;,x2&#x200B;,&#x2026; of real numbers is said to converge if there exists a real number xxx such that for all &#x3F5;&gt;0\\epsilon &gt; 0&#x3F5;&gt;0, there is an integer NNN such that &#x2223;xn&#x2212;x&#x2223;&lt;&#x3F5;\\lvert x_n-x\\rvert &lt; \\epsilon&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;&#x3F5; for all n&gt;Nn &gt; Nn&gt;N. In this case, we say that xnx_nxn&#x200B; converges to xxx, or that xxx is the limit of xnx_nxn&#x200B;, and write xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x or lim&#x2061;xn=x\\lim x_n = xlimxn&#x200B;=x (these mean the same thing). If no such number exists, the sequence is said to diverge. To make this definition concrete, let&#x2019;s consider a specific sequence: e&#x2212;1,e&#x2212;2,&#x2026;e^{-1}, e^{-2}, \\ldotse&#x2212;1,e&#x2212;2,&#x2026;. Intuitively, these numbers are getting closer to zero, so it seems right to say the sequence converges to zero. But does it satisfy the definition? Notice that there are two specific clauses that come up in the above definition, and they occur constantly in mathematical proofs and theorems: &#x201C;there exists&#x201D;: There is at least one number satisfying this condition. Denoted &#x2203;\\exists&#x2203;. &#x201C;for all&#x201D;: This has to be true in every possible case, no exceptions. Denoted &#x2200;\\forall&#x2200;. So, when we say that &#x201C;there exists a real number xxx&#x201D;, we&#x2019;re only requiring these conditions to be satisfied for the limit point; in the case of our example xn=e&#x2212;nx_n = e^{-n}xn&#x200B;=e&#x2212;n, we only need to show that it happens at x=0x=0x=0. Maybe this condition is met at other points, maybe it isn&#x2019;t. On the other hand, when we say &#x201C;for all &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0&#x201D;, the rest of the condition has to true for every single positive real number. Maybe we can find some &#x3F5;\\epsilon&#x3F5; values for which the condition is met, but it if doesn&#x2019;t hold for all of them, it isn&#x2019;t good enough. For example, suppose we consider the point x=0.001x=0.001x=0.001: does xnx_nxn&#x200B; converge to 0.001? Well, if &#x3F5;=0.002\\epsilon=0.002&#x3F5;=0.002, then yes, we can find a number NNN such that &#x2223;xn&#x2212;x&#x2223;&lt;&#x3F5;\\lvert x_n - x\\rvert &lt; \\epsilon&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;&#x3F5; for all n&gt;Nn &gt; Nn&gt;N. For example, N=6N=6N=6 works: &#x2223;e&#x2212;6&#x2212;0.001&#x2223;=0.0015&lt;0.002&#x2223;e&#x2212;7&#x2212;0.001&#x2223;=0.000088&lt;0.002&#x2026;\\begin{align*} \\lvert e^{-6}-0.001\\rvert &amp;= 0.0015 &lt; 0.002 \\\\ \\lvert e^{-7}-0.001\\rvert &amp;= 0.000088 &lt; 0.002 \\\\ \\ldots \\end{align*}&#x2223;e&#x2212;6&#x2212;0.001&#x2223;&#x2223;e&#x2212;7&#x2212;0.001&#x2223;&#x2026;&#x200B;=0.0015&lt;0.002=0.000088&lt;0.002&#x200B; However, this isn&#x2019;t enough &#x2013; we need to be able to find a suitable NNN for all positive &#x3F5;\\epsilon&#x3F5;. If &#x3F5;=0.0005\\epsilon=0.0005&#x3F5;=0.0005, for example, we would never be able find an NNN that works. Therefore, xnx_nxn&#x200B; does not converge to 0.001. One final important point to note here is that the order of these clauses matters. For every &#x3F5;\\epsilon&#x3F5;, there exists a suitable NNN. We choose &#x3F5;\\epsilon&#x3F5; first, and can then find an NNN that works for that &#x3F5;\\epsilon&#x3F5; &#x2013; we don&#x2019;t need to find an NNN that works for all &#x3F5;\\epsilon&#x3F5; (and in fact, there typically isn&#x2019;t such a NNN). If &#x3F5;=0.001\\epsilon=0.001&#x3F5;=0.001, then N=7N=7N=7 works. If &#x3F5;=0.00000000000000000000000001\\epsilon=0.00000000000000000000000001&#x3F5;=0.00000000000000000000000001, we are going to have to choose a much larger NNN (but we can always find such an NNN). Finding NNN is fairly simple in this case; we can simply take the log of both sides: e&#x2212;n&lt;&#x3F5;&#x2212;n&lt;log&#x2061;&#x3F5;n&gt;&#x2212;log&#x2061;&#x3F5;.\\begin{align*} e^{-n} &amp;&lt; \\epsilon\\\\ -n &amp;&lt; \\log \\epsilon\\\\ n &amp;&gt; -\\log \\epsilon. \\end{align*}e&#x2212;n&#x2212;nn&#x200B;&lt;&#x3F5;&lt;log&#x3F5;&gt;&#x2212;log&#x3F5;.&#x200B; So, N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309;, where &#x2308;&#x22C5;&#x2309;\\lceil \\cdot \\rceil&#x2308;&#x22C5;&#x2309; denotes the ceiling function (i.e., round up the number inside to the next integer). To make things even more concrete, one could think of N(&#x3F5;)N(\\epsilon)N(&#x3F5;) as a function: n &lt;- function(eps) {ceiling(-log(eps))} n(0.001) ## [1] 7 n(0.0000001) ## [1] 17 n(0.00000000000000001) ## [1] 40 "],["structured-proofs.html", "4.2 Structured proofs", " 4.2 Structured proofs That was a lot of preliminary stuff, but it&#x2019;s very important to understand definitions before you move on to proofs. Now, let&#x2019;s prove that e&#x2212;n&#x2192;0e^{-n} \\to 0e&#x2212;n&#x2192;0. What, specifically, do we need to prove? That for all &#x3F5;&gt;0\\epsilon &gt; 0&#x3F5;&gt;0, there is an integer NNN such that &#x2223;e&#x2212;n&#x2223;&lt;&#x3F5;\\lvert e^{-n}\\rvert &lt; \\epsilon&#x2223;e&#x2212;n&#x2223;&lt;&#x3F5; for all n&gt;Nn &gt; Nn&gt;N. So, we need to start with &#x201C;Let &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0&#x201D; (this is how a lot of proofs start): Proof. Let &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0, and let N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309;. Then for all n&gt;Nn &gt; Nn&gt;N, &#x2223;e&#x2212;n&#x2223;=e&#x2212;ne&#x2212;n&#xA0;is&#xA0;always&#xA0;positive&lt;e&#x2212;Ne&#x2212;n&#xA0;is&#xA0;strictly&#xA0;decreasing=exp&#x2061;{&#x2212;&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;}Definition&#xA0;of&#xA0;N&#x2264;elog&#x2061;&#x3F5;e&#x2212;n&#xA0;decreasing=&#x3F5;\\begin{alignat*}{2} \\lvert e^{-n}\\rvert &amp;= e^{-n} &amp;\\hspace{4em}&amp; e^{-n} \\text{ is always positive} \\\\ &amp;&lt; e^{-N} &amp;&amp; e^{-n} \\text{ is strictly decreasing}\\\\ &amp;= \\exp\\{-\\lceil-\\log \\epsilon\\rceil\\} &amp;&amp; \\text{Definition of } N \\\\ &amp;\\le e^{\\log \\epsilon} &amp;&amp; e^{-n} \\text{ decreasing} \\\\ &amp;= \\epsilon \\end{alignat*}&#x2223;e&#x2212;n&#x2223;&#x200B;=e&#x2212;n&lt;e&#x2212;N=exp{&#x2212;&#x2308;&#x2212;log&#x3F5;&#x2309;}&#x2264;elog&#x3F5;=&#x3F5;&#x200B;&#x200B;e&#x2212;n&#xA0;is&#xA0;always&#xA0;positivee&#x2212;n&#xA0;is&#xA0;strictly&#xA0;decreasingDefinition&#xA0;of&#xA0;Ne&#x2212;n&#xA0;decreasing&#x200B; Thus, xn&#x2192;0\\mathbf{x}_n \\to 0xn&#x200B;&#x2192;0. The left column is a series of statements or claims; this is the main logic of what&#x2019;s happening in the proof. Chaining all the lines together, we have &#x2223;e&#x2212;n&#x2223;&lt;&#x3F5;\\lvert e^{-n}\\rvert &lt; \\epsilon&#x2223;e&#x2212;n&#x2223;&lt;&#x3F5;. Thus, we&#x2019;ve done what we needed to do: given any positive &#x3F5;\\epsilon&#x3F5;, I can find always find an NNN that meets the requirement. Thus, e&#x2212;n&#x2192;0e^{-n} \\to 0e&#x2212;n&#x2192;0 by the definition of convergence. The right column provides the justification for each step. For example, in the fourth line we claimed that exp&#x2061;{&#x2212;&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;}&#x2264;elog&#x2061;&#x3F5;\\exp\\{-\\lceil-\\log \\epsilon\\rceil\\} \\le e^{\\log \\epsilon}exp{&#x2212;&#x2308;&#x2212;log&#x3F5;&#x2309;}&#x2264;elog&#x3F5;. How do we know that this is true? Well, &#x2308;x&#x2309;&#x2265;x\\lceil x \\rceil \\ge x&#x2308;x&#x2309;&#x2265;x since the operation involves rounding up, and e&#x2212;xe^{-x}e&#x2212;x is a decreasing function of xxx. So by replacing the argument to e&#x2212;xe^{-x}e&#x2212;x (i.e., &#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;\\lceil-\\log \\epsilon\\rceil&#x2308;&#x2212;log&#x3F5;&#x2309;) with something smaller (i.e., &#x2212;log&#x2061;&#x3F5;-\\log \\epsilon&#x2212;log&#x3F5;), the result must be larger (or equal, since &#x2212;log&#x2061;&#x3F5;-\\log\\epsilon&#x2212;log&#x3F5; could be an integer already). How far to go with these justification depends on what the reader of the proof would likely consider obvious, and is therefore a judgment call. For example, we could include a proof of the fact that e&#x2212;xe^{-x}e&#x2212;x is a decreasing function of xxx, but in my judgment this is going off on a bit of a tangent that distracts from the main point of the proof. In a similar fashion, I didn&#x2019;t even provide a justification for the final step, but to be really thorough, I could have added that exp&#x2061;()\\exp()exp() and log&#x2061;()\\log()log() are inverse functions. In general, for the purposes of a course, you should err on the side of being very thorough and justifying every step. In a paper or thesis, however, going into this level of detail for every proof is probably unnecessary as the audience (other people with PhDs in statistics or biostatistics) will probably find many of the steps obvious and not requiring justification. One final comment: as the above proof should hopefully make clear, a final proof does not describe one&#x2019;s thought process in terms of how you arrived at the result. Clearly I didn&#x2019;t know what I should set NNN equal to until I&#x2019;d worked through the math to solve for nnn in the previous section. If you&#x2019;re reading the proof for the first time, the line &#x201C;let N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309;&#x201D; is going to seem mysterious; where did that come from? Keep in mind that we&#x2019;re writing a proof, not a novel of self-discovery. The point is to construct an iron-clad, rigorous argument, not to communicate our thoughts and feelings and realizations. It might not be immediately obvious why I am letting N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309;, but it should be completely obvious that I can set N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309;. One can certainly write about the thought process and intuition behind the proof, but the place for this is outside the formal mathematical proof &#x2013; we don&#x2019;t want to mix formal logic with informal intuition. "],["convergence.html", "4.3 Convergence", " 4.3 Convergence Let&#x2019;s go through a few more proofs involving convergence (refer back to the definition at needed). For example, you might find the &#x201C;there exists a real number xxx&#x201D; clause in the definition of convergence unsatisfying, as it leaves open the possibility that many such numbers exist and a sequence might converge to lots of different things. However, this is in fact not possible. Theorem. If xn&#x2192;ax_n \\to axn&#x200B;&#x2192;a and xn&#x2192;bx_n \\to bxn&#x200B;&#x2192;b, then a=b. Proof. Let &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0. &#x2203;Na:n&gt;Na&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;a&#x2223;&lt;&#x3F5;2xn&#x2192;a&#x2203;Nb:n&gt;Nb&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;b&#x2223;&lt;&#x3F5;2xn&#x2192;b\\begin{alignat*}{2} \\exists N_a: n &gt; N_a &amp;\\implies \\lvert x_n - a\\rvert &lt; \\tfrac{\\epsilon}{2} &amp;\\hspace{4em}&amp; x_n \\to a \\\\ \\exists N_b: n &gt; N_b &amp;\\implies \\lvert x_n - b\\rvert &lt; \\tfrac{\\epsilon}{2} &amp;&amp; x_n \\to b \\end{alignat*}&#x2203;Na&#x200B;:n&gt;Na&#x200B;&#x2203;Nb&#x200B;:n&gt;Nb&#x200B;&#x200B;&#x27F9;&#x2223;xn&#x200B;&#x2212;a&#x2223;&lt;2&#x3F5;&#x200B;&#x27F9;&#x2223;xn&#x200B;&#x2212;b&#x2223;&lt;2&#x3F5;&#x200B;&#x200B;&#x200B;xn&#x200B;&#x2192;axn&#x200B;&#x2192;b&#x200B; Thus, for all n&gt;N=max&#x2061;(Na,Nb)n &gt; N = \\max(N_a, N_b)n&gt;N=max(Na&#x200B;,Nb&#x200B;), we have &#x2223;a&#x2212;b&#x2223;=&#x2223;a&#x2212;xn+xn&#x2212;b&#x2223;&#x2264;&#x2223;xn&#x2212;a&#x2223;+&#x2223;xn&#x2212;b&#x2223;Triangle&#xA0;inequality&lt;&#x3F5;\\begin{alignat*}{2} \\lvert a - b\\rvert &amp;= \\lvert a - x_n + x_n - b\\rvert \\\\ &amp;\\le \\lvert x_n - a\\rvert + \\lvert x_n - b\\rvert &amp;\\hspace{4em}&amp; \\text{Triangle inequality} \\\\ &amp;&lt; \\epsilon \\end{alignat*}&#x2223;a&#x2212;b&#x2223;&#x200B;=&#x2223;a&#x2212;xn&#x200B;+xn&#x200B;&#x2212;b&#x2223;&#x2264;&#x2223;xn&#x200B;&#x2212;a&#x2223;+&#x2223;xn&#x200B;&#x2212;b&#x2223;&lt;&#x3F5;&#x200B;&#x200B;Triangle&#xA0;inequality&#x200B; &#x2234;a=b\\therefore a=b&#x2234;a=b In the above proof, the symbol &#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;\\implies&#x27F9; means &#x201C;implies&#x201D; and the symbol &#x2234;\\therefore&#x2234; means &#x201C;therefore&#x201D; (writing out the words is fine too, I&#x2019;m just introducing the symbols because I occasionally use them in class). This proof introduces a standard technique that comes up often: since xn&#x2192;ax_n \\to axn&#x200B;&#x2192;a, I can get xnx_nxn&#x200B; as close to aaa as I want. I could find an nnn such that &#x2223;xn&#x2212;a&#x2223;&lt;&#x3F5;\\lvert x_n-a\\rvert &lt; \\epsilon&#x2223;xn&#x200B;&#x2212;a&#x2223;&lt;&#x3F5;, but why stop there? I can keep going and get &#x2223;xn&#x2212;a&#x2223;&lt;&#x3F5;/2\\lvert x_n-a\\rvert &lt; \\epsilon/2&#x2223;xn&#x200B;&#x2212;a&#x2223;&lt;&#x3F5;/2, which ends up making the rest of the proof more clear. I could keep going even further and get &#x2223;xn&#x2212;a&#x2223;&lt;&#x3F5;/1000000\\lvert x_n-a\\rvert &lt; \\epsilon/1000000&#x2223;xn&#x200B;&#x2212;a&#x2223;&lt;&#x3F5;/1000000 if I wanted, but this isn&#x2019;t necessary for what I&#x2019;m trying to show. Summarizing the above proof, if xn&#x2192;ax_n \\to axn&#x200B;&#x2192;a and xn&#x2192;bx_n \\to bxn&#x200B;&#x2192;b, then for any positive number &#x3F5;\\epsilon&#x3F5;, it must be the case that &#x2223;a&#x2212;b&#x2223;&lt;&#x3F5;\\lvert a-b\\rvert &lt; \\epsilon&#x2223;a&#x2212;b&#x2223;&lt;&#x3F5;. In other words, they have to be equal (a&#x2212;b=0a-b=0a&#x2212;b=0). Here, the triangle inequality states that &#x2223;x+y&#x2223;&#x2264;&#x2223;x&#x2223;+&#x2223;y&#x2223;.\\lvert x+y\\rvert \\le \\lvert x\\rvert + \\lvert y\\rvert.&#x2223;x+y&#x2223;&#x2264;&#x2223;x&#x2223;+&#x2223;y&#x2223;. Applied to line three, this gives &#x2223;a&#x2212;xn+xn&#x2212;b&#x2223;&#x2264;&#x2223;a&#x2212;xn&#x2223;+&#x2223;xn&#x2212;b&#x2223;&#x2223;a&#x2212;b&#x2223;&lt;&#x3F5;2+&#x3F5;2.\\begin{align*} \\lvert a - x_n + x_n - b\\rvert &amp;\\le \\lvert a-x_n\\rvert + \\lvert x_n-b\\rvert \\\\ \\lvert a-b\\rvert &amp;&lt; \\tfrac{\\epsilon}{2} + \\tfrac{\\epsilon}{2}. \\end{align*}&#x2223;a&#x2212;xn&#x200B;+xn&#x200B;&#x2212;b&#x2223;&#x2223;a&#x2212;b&#x2223;&#x200B;&#x2264;&#x2223;a&#x2212;xn&#x200B;&#x2223;+&#x2223;xn&#x200B;&#x2212;b&#x2223;&lt;2&#x3F5;&#x200B;+2&#x3F5;&#x200B;.&#x200B; Hopefully the logic here is clear and all the steps of the proof make sense (if not, feel free to come by my office). You may very well be thinking &#x201C;I understand the proof, but I could never come up with that!&#x201D; This is normal; don&#x2019;t worry about it. Any technique that you&#x2019;ve never seen before is going to seem incredible and clever, but the more exposure to proofs you have, the more you will recognize all of the above steps as fairly standard and you will definitely be able to recognize when you need them in the future. Theorem. If xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x, then the sequence xnx_nxn&#x200B; is bounded. Proof. &#x2203;N:n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x&#x2223;&lt;1xn&#x2192;x&#x2203;r=max&#x2061;{1,&#x2223;x1&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x2212;x&#x2223;}Maximum&#xA0;of&#xA0;finite&#xA0;set&#xA0;exists\\begin{alignat*}{2} &amp;\\exists N: n &gt; N \\implies \\lvert x_n - x\\rvert &lt; 1 &amp;\\hspace{4em}&amp; x_n \\to x \\\\ &amp;\\exists r = \\max\\{1, \\lvert x_1 - x\\rvert, \\ldots, \\lvert x_N - x\\rvert\\} &amp;&amp; \\text{Maximum of finite set exists}\\\\ \\end{alignat*}&#x200B;&#x2203;N:n&gt;N&#x27F9;&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;1&#x2203;r=max{1,&#x2223;x1&#x200B;&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x200B;&#x2212;x&#x2223;}&#x200B;&#x200B;xn&#x200B;&#x2192;xMaximum&#xA0;of&#xA0;finite&#xA0;set&#xA0;exists&#x200B; Therefore, {xn}n=1&#x221E;\\{x_n\\}_{n=1}^\\infty{xn&#x200B;}n=1&#x221E;&#x200B; is bounded (above by x+rx+rx+r, below by x&#x2212;rx-rx&#x2212;r). This is a fairly simple proof, but it illustrates a few important points. First, the choice of &#x201C;1&#x201D; in the first line is completely arbitrary; I could have chosen any number. Second, and more importantly, the heart of this proof is the second line, where we establish that there is a number rrr that bounds &#x2223;xn&#x2212;x&#x2223;\\lvert x_n-x\\rvert&#x2223;xn&#x200B;&#x2212;x&#x2223; and therefore also bounds xnx_nxn&#x200B;. However, it is important to recognize that this is a claim (of existence), and it requires a justification. It is very easy in an unstructured proof to just say &#x201C;let rrr by the maximum of {1,&#x2223;x1&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x2212;x&#x2223;}\\{1, \\lvert x_1 - x\\rvert, \\ldots, \\lvert x_N - x\\rvert\\}{1,&#x2223;x1&#x200B;&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x200B;&#x2212;x&#x2223;}&#x201D;. But how do you know that this maximum exists? If the set were infinite, we wouldn&#x2019;t know this. For example, we saw earlier that e&#x2212;n&#x2192;0e^{-n} \\to 0e&#x2212;n&#x2192;0, so by our proof, e&#x2212;1,e&#x2212;2,&#x2026;e^{-1}, e^{-2}, \\ldotse&#x2212;1,e&#x2212;2,&#x2026; is bounded. However, consider the set {&#x2026;,e&#x2212;(&#x2212;2),e&#x2212;(&#x2212;1),e&#x2212;0,e&#x2212;1,e&#x2212;2,&#x2026;}\\{\\ldots, e^{-(-2)}, e^{-(-1)}, e^{-0}, e^{-1}, e^{-2}, \\ldots\\}{&#x2026;,e&#x2212;(&#x2212;2),e&#x2212;(&#x2212;1),e&#x2212;0,e&#x2212;1,e&#x2212;2,&#x2026;}. This sequence converges as n&#x2192;&#x221E;n \\to \\inftyn&#x2192;&#x221E;, but because there is an infinite collection of numbers leading up to xNx_NxN&#x200B;, our proof above doesn&#x2019;t work &#x2013; we don&#x2019;t know that this set has a maximum (and indeed, it doesn&#x2019;t have a maximum, and the set isn&#x2019;t bounded). This is a common way in which it is easy to skip steps in an unstructured proof. Of course, one could still write &#x201C;Let r=max&#x2061;{1,&#x2223;x1&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x2212;x&#x2223;}r =\\max \\{1, \\lvert x_1 - x\\rvert, \\ldots, \\lvert x_N - x\\rvert\\}r=max{1,&#x2223;x1&#x200B;&#x2212;x&#x2223;,&#x2026;,&#x2223;xN&#x200B;&#x2212;x&#x2223;}&#x201D; in a structured proof, but it would be immediately clear that the right hand column is empty and that this statement has not been justified. Now, sometimes it&#x2019;s fine to say &#x201C;let&#x201D; without a justification: writing &#x201C;Let &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0&#x201D; and justifying it with &#x201C;positive numbers exist&#x201D; is pedantic. Going back to an earlier proof, one could argue that letting N=&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;N = \\lceil -\\log \\epsilon\\rceilN=&#x2308;&#x2212;log&#x3F5;&#x2309; should be justified in the sense that this only exists if &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0 (which it is), and furthermore we should be more careful in the definition: N=max&#x2061;{1,&#x2308;&#x2212;log&#x2061;&#x3F5;&#x2309;}N = \\max\\{1, \\lceil -\\log \\epsilon\\rceil\\}N=max{1,&#x2308;&#x2212;log&#x3F5;&#x2309;} is more technically sound (if &#x3F5;\\epsilon&#x3F5; is very large, NNN could be negative according to our original definition). It never hurts to think about these things, but at the same time, it&#x2019;s something of a judgment call and I felt that going into these justifications was a distraction from the main idea. On the other hand, the &#x201C;maximum of a finite set&#x201D; justification is quite important because it&#x2019;s really the main idea of the proof. Now would be a good time to try proving things on your own. Here are three theorems to start with. Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x and yn&#x2192;yy_n \\to yyn&#x200B;&#x2192;y. Then xn+yn&#x2192;x+yx_n + y_n \\to x + yxn&#x200B;+yn&#x200B;&#x2192;x+y. Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x and yn&#x2192;yy_n \\to yyn&#x200B;&#x2192;y. Then xnyn&#x2192;xyx_n y_n \\to xyxn&#x200B;yn&#x200B;&#x2192;xy. Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x, with xn&#x2260;0x_n \\ne 0xn&#x200B;&#xE020;=0 for all nnn and x&#x2260;0x \\ne 0x&#xE020;=0. Then 1/xn&#x2192;1/x1/x_n \\to 1/x1/xn&#x200B;&#x2192;1/x. For the third theorem, is xn&#x2260;0x_n \\ne 0xn&#x200B;&#xE020;=0 actually required? If we know that x&#x2260;0x \\ne 0x&#xE020;=0, can we still have xn=0x_n = 0xn&#x200B;=0? See here for solutions, although you should definitely try proving them on your own first before looking at the solutions. "],["analysis-continuity.html", "4.4 Continuity", " 4.4 Continuity Another major concept in analysis is that of continuity (again, we came across this earlier, but it&#x2019;s worth revisiting this concept not that we&#x2019;ve formally defined limits and convergence). Suppose we have a function fff and a convergent sequence xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x. Do we know that as f(xn)&#x2192;f(x)f(x_n) \\to f(x)f(xn&#x200B;)&#x2192;f(x)? The answer is that no, this doesn&#x2019;t always happen. Only some functions have this property, and those functions are said to be &#x201C;continuous&#x201D;. Below is the formal definition. Definition. A function fff is said to be continuous at the point x0x_0x0&#x200B; if for every &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0, there exists &#x3B4;&gt;0\\delta &gt; 0&#x3B4;&gt;0 such that &#x2223;f(x)&#x2212;f(x0)&#x2223;&lt;&#x3F5;\\lvert f(x) - f(x_0)\\rvert &lt; \\epsilon&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3F5; for all x:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;x: \\lvert x-x_0\\rvert &lt; \\deltax:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;. If fff is continuous at every point in its domain, then the entire function fff is said to be continuous. This is similar to the concept of convergent sequences, except now instead of a countable sequence of points x1,x2,&#x2026;x_1, x_2, \\ldotsx1&#x200B;,x2&#x200B;,&#x2026;, we are concerned with all the points in the &#x201C;neighborhood&#x201D; {x:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;}\\{x: \\lvert x-x_0\\rvert &lt; \\delta\\}{x:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;}; that is, the points near x0x_0x0&#x200B;. Many of the techniques that we encountered earlier with convergence are very similar to the techniques one uses with continuity. These techniques are often referred to as &#x201C;delta-epsilon&#x201D; techniques. For example, the techniques used in the following proof should look fairly familiar by now. Theorem. Suppose xn&#x2192;x0x_n \\to x_0xn&#x200B;&#x2192;x0&#x200B; and fff is continuous at x0x_0x0&#x200B;. Then f(xn)&#x2192;f(x0)f(x_n) \\to f(x_0)f(xn&#x200B;)&#x2192;f(x0&#x200B;). Proof. Let &#x3F5;&gt;0\\epsilon&gt; 0&#x3F5;&gt;0. &#x2203;&#x3B4;:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(x)&#x2212;f(x0)&#x2223;&lt;&#x3F5;f&#xA0;continuous&#xA0;at&#xA0;x0&#x2203;N:n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x0&#x2223;&lt;&#x3B4;xn&#x2192;x0\\begin{alignat*}{2} &amp;\\exists \\delta: \\lvert x-x_0\\rvert &lt; \\delta \\implies \\lvert f(x) - f(x_0)\\rvert &lt; \\epsilon&amp;\\hspace{4em}&amp; f \\text{ continuous at } x_0 \\\\ &amp;\\exists N: n &gt; N \\implies \\lvert x_n - x_0\\rvert &lt; \\delta &amp;&amp; x_n \\to x_0 \\end{alignat*}&#x200B;&#x2203;&#x3B4;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;&#x27F9;&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3F5;&#x2203;N:n&gt;N&#x27F9;&#x2223;xn&#x200B;&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;&#x200B;&#x200B;f&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;xn&#x200B;&#x2192;x0&#x200B;&#x200B; Therefore, n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(xn)&#x2212;f(x0)&#x2223;&lt;&#x3F5;n &gt; N \\implies \\lvert f(x_n) - f(x_0)\\rvert &lt; \\epsilonn&gt;N&#x27F9;&#x2223;f(xn&#x200B;)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3F5;. One important thing to note here is that the order of these steps is important. Students often switch the order of the first two lines in this proof, but this makes no sense. The claim that &#x2203;N:n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x0&#x2223;&lt;&#x3B4;\\exists N: n &gt; N \\implies \\lvert x_n - x_0\\rvert &lt; \\delta&#x2203;N:n&gt;N&#x27F9;&#x2223;xn&#x200B;&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4; is meaningless if &#x3B4;\\delta&#x3B4; hasn&#x2019;t been defined yet. This isn&#x2019;t just semantics: if you were trying to determine how large nnn had to be in order to ensure that f(xn)f(x_n)f(xn&#x200B;) is within a certain tolerance of f(x0)f(x_0)f(x0&#x200B;), you couldn&#x2019;t start by finding NNN. Without using continuity first, you&#x2019;d have no idea how close xnx_nxn&#x200B; must be to x0x_0x0&#x200B; in order to ensure that f(xn)f(x_n)f(xn&#x200B;) is within &#x3F5;\\epsilon&#x3F5; of f(x0)f(x_0)f(x0&#x200B;). It is worth noting that we can actually make the above theorem into an &#x201C;if and only if&#x201D; statement, and thus, an equivalent definition of continuity, but we would have to add the condition that f(xn)&#x2192;f(x)f(x_n) \\to f(x)f(xn&#x200B;)&#x2192;f(x) for all sequences xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x. For example, a function could satisfy f(xn)&#x2192;f(x0)f(x_n) \\to f(x_0)f(xn&#x200B;)&#x2192;f(x0&#x200B;) for increasing sequences xn&#x2197;x0x_n \\nearrow x_0xn&#x200B;&#x2197;x0&#x200B; but not for decreasing sequences xn&#x2198;x0x_n \\searrow x_0xn&#x200B;&#x2198;x0&#x200B;; such functions are not continuous at x0x_0x0&#x200B;. Here are some additional proof exercises related to continuity for you to practice with. Note that the sum and product proofs are very similar to the corresponding proofs for sequences; however, they are still useful exercises if you&#x2019;ve never done delta-epsilon proofs before. Theorem. Let the functions fff and ggg be continuous at x0x_0x0&#x200B;. Then h=f+gh = f + gh=f+g is continuous at x0x_0x0&#x200B;. Theorem. Let the functions fff and ggg be continuous at x0x_0x0&#x200B;. Then h=f&#x22C5;gh = f \\cdot gh=f&#x22C5;g is continuous at x0x_0x0&#x200B;. Theorem. Let the function fff be continuous at x0x_0x0&#x200B; and the function ggg be continuous at f(x0)f(x_0)f(x0&#x200B;). Then h(x)=g(f(x))h(x) = g(f(x))h(x)=g(f(x)) is continuous at x0x_0x0&#x200B;. Exercise. Write an R function n(eps) that returns the smallest NNN for which n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(xn)&#x2212;f(x0)&#x2223;&lt;&#x3F5;n &gt; N \\implies \\lvert f(x_n)-f(x_0)\\rvert &lt; \\epsilonn&gt;N&#x27F9;&#x2223;f(xn&#x200B;)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3F5; for xn=21/nx_n = 2^{1/n}xn&#x200B;=21/n and f(x)=exf(x) = e^xf(x)=ex. See here for solutions, although you should definitely try proving them on your own first before looking at the solutions. "],["analysis-solutions.html", "4.5 Solutions", " 4.5 Solutions Convergence Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x and yn&#x2192;yy_n \\to yyn&#x200B;&#x2192;y. Then xn+yn&#x2192;x+yx_n + y_n \\to x + yxn&#x200B;+yn&#x200B;&#x2192;x+y. Proof. Let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;Nx:n&gt;Na&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x&#x2223;&lt;&#x3F5;2xn&#x2192;x2&#x25EF;&#x2203;Ny:n&gt;Ny&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;yn&#x2212;y&#x2223;&lt;&#x3F5;2yn&#x2192;y\\begin{alignat*}{2} \\text{\\textcircled{1}} \\quad \\exists N_x: n&gt;N_a \\implies \\lvert x_n - x\\rvert &amp;&lt; \\tfrac{\\epsilon}{2} &amp;\\hspace{4em}&amp; x_n \\to x \\\\ \\text{\\textcircled{2}} \\quad \\exists N_y: n&gt;N_y \\implies \\lvert y_n - y\\rvert &amp;&lt; \\tfrac{\\epsilon}{2} &amp;&amp; y_n \\to y \\end{alignat*}1&#x25EF;&#x2203;Nx&#x200B;:n&gt;Na&#x200B;&#x27F9;&#x2223;xn&#x200B;&#x2212;x&#x2223;2&#x25EF;&#x2203;Ny&#x200B;:n&gt;Ny&#x200B;&#x27F9;&#x2223;yn&#x200B;&#x2212;y&#x2223;&#x200B;&lt;2&#x3F5;&#x200B;&lt;2&#x3F5;&#x200B;&#x200B;&#x200B;xn&#x200B;&#x2192;xyn&#x200B;&#x2192;y&#x200B; Thus, for all n&gt;N=max&#x2061;(Nx,Ny)n&gt;N=\\max(N_x,N_y)n&gt;N=max(Nx&#x200B;,Ny&#x200B;), we have &#x2223;xn+yn&#x2212;(x+y)&#x2223;&#x2264;&#x2223;xn&#x2212;x&#x2223;+&#x2223;yn&#x2212;y&#x2223;Triangle&#xA0;inequality&lt;&#x3F5;1&#x25EF;,2&#x25EF;\\begin{alignat*}{2} \\lvert x_n + y_n - (x+y)\\rvert &amp;\\le \\lvert x_n - x\\rvert + \\lvert y_n - y\\rvert &amp;\\hspace{6em}&amp; \\text{Triangle inequality} \\\\ &amp;&lt; \\epsilon&amp;&amp; \\text{\\textcircled{1}}, \\text{\\textcircled{2}} \\end{alignat*}&#x2223;xn&#x200B;+yn&#x200B;&#x2212;(x+y)&#x2223;&#x200B;&#x2264;&#x2223;xn&#x200B;&#x2212;x&#x2223;+&#x2223;yn&#x200B;&#x2212;y&#x2223;&lt;&#x3F5;&#x200B;&#x200B;Triangle&#xA0;inequality1&#x25EF;,2&#x25EF;&#x200B; &#xA0; Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x and yn&#x2192;yy_n \\to yyn&#x200B;&#x2192;y. Then xnyn&#x2192;xyx_n y_n \\to xyxn&#x200B;yn&#x200B;&#x2192;xy. Proof. First, let&#x2019;s establish an identity: xnyn&#x2212;xy=xnyn&#x2212;xny+xny&#x2212;xy=xn(yn&#x2212;y)+y(xn&#x2212;x)=(xn&#x2212;x+x)(yn&#x2212;y)+y(xn&#x2212;x)=(xn&#x2212;x)(yn&#x2212;y)+x(yn&#x2212;y)+y(xn&#x2212;x)\\begin{align*} x_n y_n - xy &amp;= x_n y_n - x_n y + x_n y - xy \\\\ &amp;= x_n (y_n - y) + y(x_n - x) \\\\ &amp;= (x_n - x + x)(y_n - y) + y(x_n - x) \\\\ &amp;= (x_n - x)(y_n - y) + x(y_n -y) + y(x_n - x) \\end{align*}xn&#x200B;yn&#x200B;&#x2212;xy&#x200B;=xn&#x200B;yn&#x200B;&#x2212;xn&#x200B;y+xn&#x200B;y&#x2212;xy=xn&#x200B;(yn&#x200B;&#x2212;y)+y(xn&#x200B;&#x2212;x)=(xn&#x200B;&#x2212;x+x)(yn&#x200B;&#x2212;y)+y(xn&#x200B;&#x2212;x)=(xn&#x200B;&#x2212;x)(yn&#x200B;&#x2212;y)+x(yn&#x200B;&#x2212;y)+y(xn&#x200B;&#x2212;x)&#x200B; Now, let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;Nx:n&gt;Nx&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x&#x2223;&lt;min&#x2061;(&#x3F5;3,&#x3F5;3&#x2223;y&#x2223;)xn&#x2192;x2&#x25EF;&#x2203;Ny:n&gt;Ny&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;yn&#x2212;y&#x2223;&lt;min&#x2061;(&#x3F5;3,&#x3F5;3&#x2223;x&#x2223;)yn&#x2192;y\\begin{alignat*}{2} \\text{\\textcircled{1}} \\quad \\exists N_x: n&gt;N_x \\implies &amp;\\lvert x_n - x\\rvert &lt; \\min\\left(\\sqrt{\\frac{\\epsilon}{3}}, \\frac{\\epsilon}{3\\lvert y\\rvert}\\right) &amp;\\hspace{4em}&amp; x_n \\to x \\\\ \\text{\\textcircled{2}} \\quad \\exists N_y: n&gt;N_y \\implies &amp;\\lvert y_n - y\\rvert &lt; \\min\\left(\\sqrt{\\frac{\\epsilon}{3}}, \\frac{\\epsilon}{3\\lvert x\\rvert}\\right) &amp;&amp; y_n \\to y \\end{alignat*}1&#x25EF;&#x2203;Nx&#x200B;:n&gt;Nx&#x200B;&#x27F9;2&#x25EF;&#x2203;Ny&#x200B;:n&gt;Ny&#x200B;&#x27F9;&#x200B;&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;min(3&#x3F5;&#x200B;&#x200B;,3&#x2223;y&#x2223;&#x3F5;&#x200B;)&#x2223;yn&#x200B;&#x2212;y&#x2223;&lt;min(3&#x3F5;&#x200B;&#x200B;,3&#x2223;x&#x2223;&#x3F5;&#x200B;)&#x200B;&#x200B;xn&#x200B;&#x2192;xyn&#x200B;&#x2192;y&#x200B; Thus, for all n&gt;N=max&#x2061;(Nx,Ny)n&gt;N=\\max(N_x,N_y)n&gt;N=max(Nx&#x200B;,Ny&#x200B;), we have &#x2223;xnyn&#x2212;xy&#x2223;=&#x2223;(xn&#x2212;x)(yn&#x2212;y)+x(yn&#x2212;y)+y(xn&#x2212;x)&#x2223;Identity&#xA0;above&#x2264;&#x2223;xn&#x2212;x&#x2223;&#x2223;yn&#x2212;y&#x2223;+&#x2223;x&#x2223;&#x2223;yn&#x2212;y&#x2223;+&#x2223;y&#x2223;&#x2223;xn&#x2212;x&#x2223;Triangle&#xA0;inequality&lt;&#x3F5;3+&#x3F5;3+&#x3F5;31&#x25EF;,2&#x25EF;=&#x3F5;\\begin{alignat*}{2} \\lvert x_n y_n - xy\\rvert &amp;= \\lvert(x_n - x)(y_n - y) + x(y_n -y) + y(x_n - x)\\rvert &amp;\\hspace{4em}&amp; \\text{Identity above} \\\\ &amp;\\le \\lvert x_n-x\\rvert\\lvert y_n-y\\rvert + \\lvert x\\rvert\\lvert y_n-y\\rvert + \\lvert y\\rvert\\lvert x_n-x\\rvert &amp;&amp; \\text{Triangle inequality} \\\\ &amp;&lt; \\tfrac{\\epsilon}{3} + \\tfrac{\\epsilon}{3} + \\tfrac{\\epsilon}{3} &amp;&amp; \\text{\\textcircled{1}}, \\text{\\textcircled{2}} \\\\ &amp;= \\epsilon \\end{alignat*}&#x2223;xn&#x200B;yn&#x200B;&#x2212;xy&#x2223;&#x200B;=&#x2223;(xn&#x200B;&#x2212;x)(yn&#x200B;&#x2212;y)+x(yn&#x200B;&#x2212;y)+y(xn&#x200B;&#x2212;x)&#x2223;&#x2264;&#x2223;xn&#x200B;&#x2212;x&#x2223;&#x2223;yn&#x200B;&#x2212;y&#x2223;+&#x2223;x&#x2223;&#x2223;yn&#x200B;&#x2212;y&#x2223;+&#x2223;y&#x2223;&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;3&#x3F5;&#x200B;+3&#x3F5;&#x200B;+3&#x3F5;&#x200B;=&#x3F5;&#x200B;&#x200B;Identity&#xA0;aboveTriangle&#xA0;inequality1&#x25EF;,2&#x25EF;&#x200B; In the construction of NNN above, note that we are assuming x,y&#x2260;0x,y \\ne 0x,y&#xE020;=0. If either is zero, the second term in the sum can simply be omitted, as the corresponding term below is zero. Theorem. Suppose xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x, with xn&#x2260;0x_n \\ne 0xn&#x200B;&#xE020;=0 for all nnn and x&#x2260;0x \\ne 0x&#xE020;=0. Then 1/xn&#x2192;1/x1/x_n \\to 1/x1/xn&#x200B;&#x2192;1/x. Proof. First, let us note that &#x2223;a&#x2212;b&#x2223;&lt;12&#x2223;b&#x2223;&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;a&#x2223;&gt;12b\\lvert a-b\\rvert &lt; \\tfrac{1}{2}\\lvert b\\rvert \\implies \\lvert a\\rvert &gt; \\tfrac{1}{2}{b}&#x2223;a&#x2212;b&#x2223;&lt;21&#x200B;&#x2223;b&#x2223;&#x27F9;&#x2223;a&#x2223;&gt;21&#x200B;b. This is fairly obvious when you think about it; to prove it, we can break the claim up into cases: The cases where b&lt;0b&lt;0b&lt;0 follow the same reasoning. Now, let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;N1:n&gt;N1&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x&#x2223;&lt;12&#x2223;x&#x2223;2&#x3F5;xn&#x2192;x2&#x25EF;&#x2203;N2:n&gt;N2&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;xn&#x2212;x&#x2223;&lt;12&#x2223;x&#x2223;xn&#x2192;x3&#x25EF;so&#xA0;that&#xA0;&#x2223;xn&#x2223;&gt;12&#x2223;x&#x2223;2&#x25EF;,see&#xA0;above\\begin{alignat*}{3} \\text{\\textcircled{1}} &amp;\\quad&amp; \\exists N_1: n&gt;N_1 \\implies \\lvert x_n - x\\rvert &amp;&lt; \\tfrac{1}{2}\\lvert x\\rvert^2\\epsilon&amp;\\hspace{8em}&amp; x_n \\to x \\\\ \\text{\\textcircled{2}} &amp;&amp; \\exists N_2: n&gt;N_2 \\implies \\lvert x_n-x\\rvert &amp;&lt; \\tfrac{1}{2}\\lvert x\\rvert &amp;&amp; x_n \\to x \\\\ \\text{\\textcircled{3}} &amp;&amp; \\text{so that } \\lvert x_n\\rvert &amp;&gt; \\tfrac{1}{2}\\lvert x\\rvert &amp;&amp; \\text{\\textcircled{2}}, \\text{see above} \\end{alignat*}1&#x25EF;2&#x25EF;3&#x25EF;&#x200B;&#x200B;&#x2203;N1&#x200B;:n&gt;N1&#x200B;&#x27F9;&#x2223;xn&#x200B;&#x2212;x&#x2223;&#x2203;N2&#x200B;:n&gt;N2&#x200B;&#x27F9;&#x2223;xn&#x200B;&#x2212;x&#x2223;so&#xA0;that&#xA0;&#x2223;xn&#x200B;&#x2223;&#x200B;&lt;21&#x200B;&#x2223;x&#x2223;2&#x3F5;&lt;21&#x200B;&#x2223;x&#x2223;&gt;21&#x200B;&#x2223;x&#x2223;&#x200B;&#x200B;xn&#x200B;&#x2192;xxn&#x200B;&#x2192;x2&#x25EF;,see&#xA0;above&#x200B; Thus, for all n&gt;N=max&#x2061;(N1,N2)n&gt;N=\\max(N_1, N_2)n&gt;N=max(N1&#x200B;,N2&#x200B;), we have &#x2223;1xn&#x2212;1x&#x2223;=&#x2223;x&#x2212;xnxnx&#x2223;&lt;2&#x2223;x&#x2223;2&#x2223;xn&#x2212;x&#x2223;3&#x25EF;&lt;&#x3F5;1&#x25EF;\\begin{alignat*}{2} \\lvert\\frac{1}{x_n} - \\frac{1}{x}\\rvert &amp;= \\lvert\\frac{x-x_n}{x_n x}\\rvert \\\\ &amp;&lt; \\frac{2}{\\lvert x\\rvert^2}\\lvert x_n-x\\rvert &amp;\\hspace{8em}&amp; \\text{\\textcircled{3}} \\\\ &amp;&lt; \\epsilon&amp;&amp; \\text{\\textcircled{1}} \\end{alignat*}&#x2223;xn&#x200B;1&#x200B;&#x2212;x1&#x200B;&#x2223;&#x200B;=&#x2223;xn&#x200B;xx&#x2212;xn&#x200B;&#x200B;&#x2223;&lt;&#x2223;x&#x2223;22&#x200B;&#x2223;xn&#x200B;&#x2212;x&#x2223;&lt;&#x3F5;&#x200B;&#x200B;3&#x25EF;1&#x25EF;&#x200B; Note that in this third theorem, the requirement that xn&#x2260;0x_n \\ne 0xn&#x200B;&#xE020;=0 is unnecessary. As we see from 3&#x25EF;\\text{\\textcircled{3}}3&#x25EF;, if xn&#x2192;xx_n \\to xxn&#x200B;&#x2192;x and x&#x2260;0x \\ne 0x&#xE020;=0, then there is an NNN such that xn&#x2260;0x_n \\ne 0xn&#x200B;&#xE020;=0 for all n&gt;Nn&gt;Nn&gt;N. Continuity The first two theorems are essentially the same as their sequence counterparts, but the differences are worth paying attention to. Theorem. Let the functions fff and ggg be continuous at x0x_0x0&#x200B;. Then h=f+gh = f + gh=f+g is continuous at x0x_0x0&#x200B;. Proof. Let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;&#x3B4;f:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;f&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(x)&#x2212;f(x0)&#x2223;&lt;&#x3F5;2f&#xA0;continuous&#xA0;at&#xA0;x02&#x25EF;&#x2203;&#x3B4;g:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;g&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;g(x)&#x2212;g(x0)&#x2223;&lt;&#x3F5;2g&#xA0;continuous&#xA0;at&#xA0;x0\\begin{alignat*}{2} \\text{\\textcircled{1}} \\quad \\exists \\delta_f: \\lvert x-x_0\\rvert &lt; \\delta_f &amp;\\implies \\lvert f(x) - f(x_0)\\rvert &lt; \\tfrac{\\epsilon}{2} &amp;\\hspace{6em}&amp; f \\text{ continuous at } x_0 \\\\ \\text{\\textcircled{2}} \\quad \\exists \\delta_g: \\lvert x-x_0\\rvert &lt; \\delta_g &amp;\\implies \\lvert g(x) - g(x_0)\\rvert &lt; \\tfrac{\\epsilon}{2} &amp;&amp; g \\text{ continuous at } x_0 \\end{alignat*}1&#x25EF;&#x2203;&#x3B4;f&#x200B;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;f&#x200B;2&#x25EF;&#x2203;&#x3B4;g&#x200B;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;g&#x200B;&#x200B;&#x27F9;&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;&lt;2&#x3F5;&#x200B;&#x27F9;&#x2223;g(x)&#x2212;g(x0&#x200B;)&#x2223;&lt;2&#x3F5;&#x200B;&#x200B;&#x200B;f&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;g&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;&#x200B; Thus, for all x:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;=min&#x2061;(&#x3B4;f,&#x3B4;g)x:\\lvert x-x_0\\rvert &lt; \\delta=\\min(\\delta_f, \\delta_g)x:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;=min(&#x3B4;f&#x200B;,&#x3B4;g&#x200B;), we have &#x2223;h(x)&#x2212;h(x0)&#x2223;=&#x2223;f(x)+g(x)&#x2212;f(x0)&#x2212;g(x0)&#x2223;Def&#xA0;h&#x2264;&#x2223;f(x)&#x2212;f(x0)&#x2223;+&#x2223;g(x)&#x2212;g(x0)&#x2223;Triangle&#xA0;inequality&#x2264;&#x3F5;1&#x25EF;,2&#x25EF;\\begin{alignat*}{2} \\lvert h(x) -h(x_0)\\rvert &amp;= \\lvert f(x) + g(x) - f(x_0) - g(x_0)\\rvert &amp;\\hspace{6em}&amp; \\text{Def } h \\\\ &amp;\\le \\lvert f(x)-f(x_0)\\rvert + \\lvert g(x)-g(x_0)\\rvert &amp;&amp; \\text{Triangle inequality} \\\\ &amp;\\le \\epsilon&amp;&amp; \\text{\\textcircled{1}}, \\text{\\textcircled{2}} \\end{alignat*}&#x2223;h(x)&#x2212;h(x0&#x200B;)&#x2223;&#x200B;=&#x2223;f(x)+g(x)&#x2212;f(x0&#x200B;)&#x2212;g(x0&#x200B;)&#x2223;&#x2264;&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;+&#x2223;g(x)&#x2212;g(x0&#x200B;)&#x2223;&#x2264;&#x3F5;&#x200B;&#x200B;Def&#xA0;hTriangle&#xA0;inequality1&#x25EF;,2&#x25EF;&#x200B; &#xA0; Theorem. Let the functions fff and ggg be continuous at x0x_0x0&#x200B;. Then h=f&#x22C5;gh = f \\cdot gh=f&#x22C5;g is continuous at x0x_0x0&#x200B;. Proof. Let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;&#x3B4;f:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;f&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(x)&#x2212;f(x0)&#x2223;&lt;&#x3F5;3+&#x3F5;3&#x2223;g(x0)&#x2223;f&#xA0;continuous&#xA0;at&#xA0;x02&#x25EF;&#x2203;&#x3B4;g:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;g&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;g(x)&#x2212;g(x0)&#x2223;&lt;&#x3F5;3+&#x3F5;3&#x2223;f(x0)&#x2223;g&#xA0;continuous&#xA0;at&#xA0;x0\\begin{alignat*}{2} \\text{\\textcircled{1}} \\quad \\exists \\delta_f: \\lvert x-x_0\\rvert &lt; \\delta_f &amp;\\implies \\lvert f(x) - f(x_0)\\rvert &lt; \\frac{\\sqrt{\\epsilon}}{3} + \\frac{\\epsilon}{3\\lvert g(x_0)\\rvert} &amp;\\hspace{3em}&amp; f \\text{ continuous at } x_0 \\\\ \\text{\\textcircled{2}} \\quad \\exists \\delta_g: \\lvert x-x_0\\rvert &lt; \\delta_g &amp;\\implies \\lvert g(x) - g(x_0)\\rvert &lt; \\frac{\\sqrt{\\epsilon}}{3} + \\frac{\\epsilon}{3\\lvert f(x_0)\\rvert} &amp;&amp; g \\text{ continuous at } x_0 \\end{alignat*}1&#x25EF;&#x2203;&#x3B4;f&#x200B;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;f&#x200B;2&#x25EF;&#x2203;&#x3B4;g&#x200B;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;g&#x200B;&#x200B;&#x27F9;&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;&lt;3&#x3F5;&#x200B;&#x200B;+3&#x2223;g(x0&#x200B;)&#x2223;&#x3F5;&#x200B;&#x27F9;&#x2223;g(x)&#x2212;g(x0&#x200B;)&#x2223;&lt;3&#x3F5;&#x200B;&#x200B;+3&#x2223;f(x0&#x200B;)&#x2223;&#x3F5;&#x200B;&#x200B;&#x200B;f&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;g&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;&#x200B; Thus, for all x:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;=min&#x2061;(&#x3B4;f,&#x3B4;g)x:\\lvert x-x_0\\rvert &lt; \\delta=\\min(\\delta_f, \\delta_g)x:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;=min(&#x3B4;f&#x200B;,&#x3B4;g&#x200B;), we have &#x2223;h(x)&#x2212;h(x0)&#x2223;=&#x2223;f(x)g(x)&#x2212;f(x0)g(x0)&#x2223;Def&#xA0;h&#x2264;&#x2223;{f(x)&#x2212;f(x0)}{g(x)&#x2212;g(x0)}&#x2223;+&#x2223;f(x0){g(x)&#x2212;g(x0)}&#x2223;+&#x2223;g(x0){f(x)&#x2212;f(x0)}&#x2223;See&#xA0;earlier&#xA0;proof&lt;&#x3F5;3+&#x3F5;3+&#x3F5;31&#x25EF;,2&#x25EF;=&#x3F5;\\begin{alignat*}{2} \\lvert h(x) - h(x_0)\\rvert &amp;= \\lvert f(x)g(x) - f(x_0)g(x_0)\\rvert &amp;\\hspace{6em}&amp; \\text{Def } h \\\\ &amp;\\le \\lvert\\{f(x) - f(x_0)\\}\\{g(x)-g(x_0)\\}\\rvert \\\\ &amp;\\qquad + \\lvert f(x_0)\\{g(x)-g(x_0)\\}\\rvert \\\\ &amp;\\qquad + \\lvert g(x_0)\\{f(x)-f(x_0)\\}\\rvert &amp;&amp; \\text{See earlier proof} \\\\ &amp;&lt; \\tfrac{\\epsilon}{3} + \\tfrac{\\epsilon}{3} + \\tfrac{\\epsilon}{3} &amp;&amp; \\text{\\textcircled{1}}, \\text{\\textcircled{2}} \\\\ &amp;= \\epsilon \\end{alignat*}&#x2223;h(x)&#x2212;h(x0&#x200B;)&#x2223;&#x200B;=&#x2223;f(x)g(x)&#x2212;f(x0&#x200B;)g(x0&#x200B;)&#x2223;&#x2264;&#x2223;{f(x)&#x2212;f(x0&#x200B;)}{g(x)&#x2212;g(x0&#x200B;)}&#x2223;+&#x2223;f(x0&#x200B;){g(x)&#x2212;g(x0&#x200B;)}&#x2223;+&#x2223;g(x0&#x200B;){f(x)&#x2212;f(x0&#x200B;)}&#x2223;&lt;3&#x3F5;&#x200B;+3&#x3F5;&#x200B;+3&#x3F5;&#x200B;=&#x3F5;&#x200B;&#x200B;Def&#xA0;hSee&#xA0;earlier&#xA0;proof1&#x25EF;,2&#x25EF;&#x200B; &#xA0; Theorem. Let the function fff be continuous at x0x_0x0&#x200B; and the function ggg be continuous at f(x0)f(x_0)f(x0&#x200B;). Then h(x)=g(f(x))h(x) = g(f(x))h(x)=g(f(x)) is continuous at x0x_0x0&#x200B;. Proof. Let &#x3F5;&gt;0\\epsilon&gt;0&#x3F5;&gt;0. 1&#x25EF;&#x2203;&#x3B7;:&#x2223;y&#x2212;f(x0)&#x2223;&lt;&#x3B7;&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;g(y)&#x2212;g(f(x0))&#x2223;&lt;&#x3F5;g&#xA0;continuous&#xA0;at&#xA0;f(x0)2&#x25EF;&#x2203;&#x3B4;:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(x)&#x2212;f(x0)&#x2223;&lt;&#x3B7;f&#xA0;continuous&#xA0;at&#xA0;x0\\begin{alignat*}{2} \\text{\\textcircled{1}} \\qquad \\exists \\eta: \\lvert y-f(x_0)\\rvert &lt; \\eta &amp;\\implies \\lvert g(y) - g(f(x_0))\\rvert &lt; \\epsilon&amp;\\hspace{4em}&amp; g \\text{ continuous at } f(x_0) \\\\ \\text{\\textcircled{2}} \\qquad \\exists \\delta: \\lvert x-x_0\\rvert &lt; \\delta &amp;\\implies \\lvert f(x) - f(x_0)\\rvert &lt; \\eta &amp;&amp; f \\text{ continuous at } x_0 \\end{alignat*}1&#x25EF;&#x2203;&#x3B7;:&#x2223;y&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3B7;2&#x25EF;&#x2203;&#x3B4;:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;&#x200B;&#x27F9;&#x2223;g(y)&#x2212;g(f(x0&#x200B;))&#x2223;&lt;&#x3F5;&#x27F9;&#x2223;f(x)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3B7;&#x200B;&#x200B;g&#xA0;continuous&#xA0;at&#xA0;f(x0&#x200B;)f&#xA0;continuous&#xA0;at&#xA0;x0&#x200B;&#x200B; Thus, for all x:&#x2223;x&#x2212;x0&#x2223;&lt;&#x3B4;x:\\lvert x-x_0\\rvert &lt; \\deltax:&#x2223;x&#x2212;x0&#x200B;&#x2223;&lt;&#x3B4;, we have &#x2223;h(x)&#x2212;h(x0)&#x2223;=&#x2223;g(f(x))&#x2212;g(f(x0))&#x2223;Def&#xA0;h&lt;&#x3F5;2&#x25EF;&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;1&#x25EF;\\begin{alignat*}{2} \\lvert h(x) - h(x_0)\\rvert &amp;= \\lvert g(f(x)) - g(f(x_0))\\rvert &amp;\\hspace{4em}&amp; \\text{Def } h \\\\ &amp;&lt; \\epsilon&amp;&amp; \\text{\\textcircled{2}} \\implies \\text{\\textcircled{1}} \\end{alignat*}&#x2223;h(x)&#x2212;h(x0&#x200B;)&#x2223;&#x200B;=&#x2223;g(f(x))&#x2212;g(f(x0&#x200B;))&#x2223;&lt;&#x3F5;&#x200B;&#x200B;Def&#xA0;h2&#x25EF;&#x27F9;1&#x25EF;&#x200B; Exercise: Write an R function n(eps) that returns the smallest NNN for which n&gt;N&#x2005;&#x200A;&#x27F9;&#x2005;&#x200A;&#x2223;f(xn)&#x2212;f(x0)&#x2223;&lt;&#x3F5;n &gt; N \\implies \\lvert f(x_n)-f(x_0)\\rvert &lt; \\epsilonn&gt;N&#x27F9;&#x2223;f(xn&#x200B;)&#x2212;f(x0&#x200B;)&#x2223;&lt;&#x3F5; for xn=21/nx_n = 2^{1/n}xn&#x200B;=21/n and f(x)=exf(x) = e^xf(x)=ex. Conceptually, this is a three-part process: Determine what xnx_nxn&#x200B; is converging to. Here, xn&#x2192;1x_n \\to 1xn&#x200B;&#x2192;1. Determine the largest value of delta that satisfies e1+&#x3B4;&#x2212;e1&lt;&#x3F5;e^{1+\\delta} - e^1 &lt; \\epsilone1+&#x3B4;&#x2212;e1&lt;&#x3F5;. Determine the smallest value of NNN such that 21/n&#x2212;1&lt;&#x3B4;2^{1/n} - 1 &lt; \\delta21/n&#x2212;1&lt;&#x3B4;. n &lt;- function(eps) { delta &lt;- log(eps + exp(1)) - 1 ceiling(1/log2(1+delta)) } Let&#x2019;s test this out and make sure it works: n(0.01) ## [1] 190 exp(2^(1/189)) - exp(1) ## 189 not good enough ## [1] 0.01000582 exp(2^(1/190)) - exp(1) ## 190 within 0.01 ## [1] 0.009952969 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
